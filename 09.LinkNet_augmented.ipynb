{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1212941825077735812, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 9214062756\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 615534763411529830\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import layers, metrics, models, optimizers\n",
    "from keras.optimizers import Adam, RMSprop,SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, concatenate, Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Version 2.2.4\n",
      "tensorflow Version 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"tensorflow Version\", tf.__version__)\n",
    "# print(\"dim_ordering:\", K.image_dim_ordering())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "img_rows, img_cols = 256, 256\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data_augmented/cvc300_train/'\n",
    "validation_dir = 'data_augmented/cvc300_validation/'\n",
    "test_dir = 'data_augmented/cvc300_test/'\n",
    "\n",
    "image_folder = 'images'\n",
    "masks_folder = 'masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://www.kaggle.com/kmader/keras-linknet\n",
    "\"\"\"\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Deconv2D, MaxPool2D, concatenate, AvgPool2D\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import add\n",
    "\n",
    "\n",
    "s_c2 = lambda fc, k, s = 1, activation='elu', **kwargs: Conv2D(fc, kernel_size = (k,k), strides= (s,s),\n",
    "                                       padding = 'same', activation = activation,\n",
    "                                       **kwargs)\n",
    "\n",
    "\n",
    "s_d2 = lambda fc, k, s = 1, activation='elu', **kwargs: Deconv2D(fc, kernel_size=(k,k), strides=(s,s), \n",
    "                                                       padding = 'same', activation=activation,\n",
    "                                                       **kwargs)\n",
    "\n",
    "\n",
    "c2 = lambda fc, k, s = 1, **kwargs: lambda x: Activation('elu')(BatchNormalization()(\n",
    "    Conv2D(fc, kernel_size = (k,k), strides= (s,s),\n",
    "           padding = 'same', activation = 'linear', **kwargs)(x)))\n",
    "\n",
    "\n",
    "d2 = lambda fc, k, s = 1, **kwargs: lambda x: Activation('elu')(BatchNormalization()(\n",
    "    Deconv2D(fc, kernel_size=(k,k), strides=(s,s), \n",
    "             padding = 'same', activation='linear', **kwargs)(x)))\n",
    "\n",
    "\n",
    "def LinkNet(img_size):\n",
    "    start_in = Input((img_size, img_size, 3), name = 'Input')\n",
    "    in_filt = c2(64, 7, 2)(start_in)\n",
    "    in_mp = MaxPool2D((3,3), strides = (2,2), padding = 'same')(in_filt)\n",
    "\n",
    "    enc1 = enc_block(64, 64)(in_mp)\n",
    "    enc2 = enc_block(64, 128)(enc1)\n",
    "\n",
    "    dec2 = dec_block(64, 128)(enc2)\n",
    "    dec2_cat = _shortcut(enc1, dec2)\n",
    "    dec1 = dec_block(64, 64)(dec2_cat)\n",
    "\n",
    "    last_out = _shortcut(dec1, in_mp)\n",
    "    \n",
    "    out_upconv = d2(32, 3, 2)(last_out)\n",
    "    out_conv = c2(32, 3)(out_upconv)\n",
    "    out = s_d2(1, 2, 2, activation = 'sigmoid')(out_conv)\n",
    "\n",
    "    model = Model(inputs = [start_in], outputs = [out])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _shortcut(input, residual):\n",
    "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
    "    \"\"\"\n",
    "    # Expand channels of shortcut to match residual.\n",
    "    # Stride appropriately to match residual (width, height)\n",
    "    # Should be int if network architecture is correctly configured.\n",
    "    input_shape = K.int_shape(input)\n",
    "    residual_shape = K.int_shape(residual)\n",
    "    stride_width = int(round(input_shape[1] / residual_shape[1]))\n",
    "    stride_height = int(round(input_shape[2] / residual_shape[2]))\n",
    "    equal_channels = input_shape[3] == residual_shape[3]\n",
    "\n",
    "    shortcut = input\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=residual_shape[3],\n",
    "                          kernel_size=(1, 1),\n",
    "                          strides=(stride_width, stride_height),\n",
    "                          padding=\"valid\",\n",
    "                          kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2(0.0001))(input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "\n",
    "\n",
    "def enc_block(m, n):\n",
    "    def block_func(x):\n",
    "        cx = c2(n, 3)(c2(n, 3, 2)(x))\n",
    "        cs1 = concatenate([AvgPool2D((2,2))(x), \n",
    "                           cx])\n",
    "        cs2 = c2(n, 3)(c2(n, 3)(cs1))\n",
    "        return concatenate([cs2, cs1])\n",
    "    return block_func\n",
    "\n",
    "\n",
    "def dec_block(m, n):\n",
    "    def block_func(x):\n",
    "        cx1 = c2(m//4, 1)(x)\n",
    "        cx2 = d2(m//4, 3, 2)(cx1)\n",
    "        return Dropout(0.1)(c2(n, 1)(cx2))\n",
    "    return block_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26112 images belonging to 1 classes.\n",
      "Found 26112 images belonging to 1 classes.\n",
      "Found 8704 images belonging to 1 classes.\n",
      "Found 8704 images belonging to 1 classes.\n",
      "Found 28 images belonging to 1 classes.\n",
      "Found 28 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "seed = 1142\n",
    "data_gen_args = dict(rescale=1. / 255, featurewise_center=True, featurewise_std_normalization=True)\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "masks_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    classes = [image_folder],\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)\n",
    "\n",
    "mask_generator = masks_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    classes = [masks_folder],\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)\n",
    "\n",
    "validation_image_generator = image_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    classes = [image_folder],\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)\n",
    "\n",
    "validation_mask_generator = masks_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    classes = [masks_folder],\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)\n",
    "\n",
    "test_image_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    classes = [image_folder],\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)\n",
    "\n",
    "test_mask_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    classes = [masks_folder],\n",
    "    color_mode='grayscale',\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = zip(image_generator, mask_generator)\n",
    "validation_generator = zip(validation_image_generator, validation_mask_generator)\n",
    "test_generator = zip(test_image_generator, test_mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_train_samples: 26112\n",
      "nb_validation_samples: 8704\n",
      "nb_test_samples: 28\n",
      "\n",
      "predict_size_train: 1632\n",
      "predict_size_validation: 544\n",
      "predict_size_test: 2\n"
     ]
    }
   ],
   "source": [
    "nb_train_samples = len(image_generator.filenames)\n",
    "nb_validation_samples = len(validation_image_generator.filenames)\n",
    "nb_test_samples = len(test_image_generator.filenames)\n",
    "\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))\n",
    "\n",
    "print(\"nb_train_samples:\", nb_train_samples)\n",
    "print(\"nb_validation_samples:\", nb_validation_samples)\n",
    "print(\"nb_test_samples:\", nb_test_samples)\n",
    "\n",
    "print(\"\\npredict_size_train:\", predict_size_train)\n",
    "print(\"predict_size_validation:\", predict_size_validation)\n",
    "print(\"predict_size_test:\", predict_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def Specificity(y_true, y_pred):\n",
    "    true_negatives = K.abs(y_pred)- K.abs(y_true)\n",
    "    return ((true_negatives+smooth)/(y_pred+ smooth))\n",
    "\n",
    "def Sensitivity(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    return ((y_pred+smooth)/ (y_true+smooth))\n",
    "\n",
    "def Jaccard_index(y_true,y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return ((intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + smooth))\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "        Only computes a batch-wise average of recall.\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "        Only computes a batch-wise average of precision.\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 9472        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36928       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 32, 32, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           average_pooling2d_1[0][0]        \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 192)  0           activation_5[0][0]               \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 128)  221312      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 16, 16, 192)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 16, 320)  0           average_pooling2d_2[0][0]        \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 128)  368768      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 128)  147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 448)  0           activation_9[0][0]               \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 16)   7184        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 16)   2320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 128)  2176        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 128)  24704       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 128)  0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_12[0][0]                  \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2064        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 16)   2320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 64)   1088        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 64)   256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64, 64, 64)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 64)   0           dropout_2[0][0]                  \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 32) 18464       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 32) 128         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 32) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 32) 9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 128, 128, 32) 128         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 128, 128, 32) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 1)  129         activation_17[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,153,601\n",
      "Trainable params: 1,151,297\n",
      "Non-trainable params: 2,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LinkNet(img_size = 256)\n",
    "model.summary()\n",
    "adam_opt = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer= adam_opt, loss=dice_coef_loss, \n",
    "              metrics=[dice_coef, 'acc','mse', Jaccard_index, Specificity, Sensitivity, precision, recall, f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1632/1632 [==============================] - 382s 234ms/step - loss: -0.1139 - dice_coef: 0.1376 - acc: 0.5349 - mean_squared_error: 0.2428 - Jaccard_index: 0.0743 - Specificity: 0.9733 - Sensitivity: 1.4329 - precision: 0.1154 - recall: 0.8062 - f1score: 0.2002 - val_loss: -0.1752 - val_dice_coef: 0.1976 - val_acc: 0.6533 - val_mean_squared_error: 0.1990 - val_Jaccard_index: 0.1100 - val_Specificity: 0.9686 - val_Sensitivity: 1.3691 - val_precision: 0.2055 - val_recall: 0.8781 - val_f1score: 0.3311\n",
      "Epoch 2/50\n",
      "1632/1632 [==============================] - 383s 235ms/step - loss: -0.1638 - dice_coef: 0.1854 - acc: 0.6690 - mean_squared_error: 0.1943 - Jaccard_index: 0.1025 - Specificity: 0.9754 - Sensitivity: 1.3659 - precision: 0.1999 - recall: 0.9523 - f1score: 0.3286 - val_loss: -0.2047 - val_dice_coef: 0.2254 - val_acc: 0.7352 - val_mean_squared_error: 0.1365 - val_Jaccard_index: 0.1274 - val_Specificity: 0.9676 - val_Sensitivity: 1.2943 - val_precision: 0.3071 - val_recall: 0.8186 - val_f1score: 0.4431\n",
      "Epoch 3/50\n",
      "1632/1632 [==============================] - 389s 238ms/step - loss: -0.1943 - dice_coef: 0.2142 - acc: 0.7101 - mean_squared_error: 0.1611 - Jaccard_index: 0.1204 - Specificity: 0.9758 - Sensitivity: 1.3150 - precision: 0.2486 - recall: 0.9668 - f1score: 0.3932 - val_loss: -0.2330 - val_dice_coef: 0.2522 - val_acc: 0.7313 - val_mean_squared_error: 0.1380 - val_Jaccard_index: 0.1448 - val_Specificity: 0.9688 - val_Sensitivity: 1.2769 - val_precision: 0.3134 - val_recall: 0.8797 - val_f1score: 0.4592- ETA: 39s - loss: -0.1922 - dice_coef: 0.2122 - acc: 0.70 - ETA: 13s - loss: -0.1937 - dice_coef: 0.2136 - acc: 0.7098 - mean_squared_err\n",
      "Epoch 4/50\n",
      "1632/1632 [==============================] - 386s 237ms/step - loss: -0.2265 - dice_coef: 0.2449 - acc: 0.7382 - mean_squared_error: 0.1317 - Jaccard_index: 0.1401 - Specificity: 0.9758 - Sensitivity: 1.2662 - precision: 0.2959 - recall: 0.9694 - f1score: 0.4509 - val_loss: -0.2699 - val_dice_coef: 0.2877 - val_acc: 0.7535 - val_mean_squared_error: 0.1104 - val_Jaccard_index: 0.1687 - val_Specificity: 0.9686 - val_Sensitivity: 1.2274 - val_precision: 0.3674 - val_recall: 0.8760 - val_f1score: 0.5138\n",
      "Epoch 5/50\n",
      "1632/1632 [==============================] - 388s 238ms/step - loss: -0.2644 - dice_coef: 0.2817 - acc: 0.7613 - mean_squared_error: 0.1039 - Jaccard_index: 0.1647 - Specificity: 0.9758 - Sensitivity: 1.2193 - precision: 0.3509 - recall: 0.9690 - f1score: 0.5124 - val_loss: -0.3041 - val_dice_coef: 0.3209 - val_acc: 0.7579 - val_mean_squared_error: 0.1010 - val_Jaccard_index: 0.1918 - val_Specificity: 0.9688 - val_Sensitivity: 1.1977 - val_precision: 0.3844 - val_recall: 0.8950 - val_f1score: 0.5345 0.9758 - Sens - ETA: 1:13 - loss: -0.2601 - dice_coef: 0.2775 - acc: 0.7600 - mean_squared_error: 0.1068 - Jaccard_index: 0.16 - ETA: 31s - loss: -0.2627 - dice_coef: 0.2800 - acc: 0.7604 - mean_squared_error: 0.1051 - Jaccard_index: 0.1636 - Specificity: 0.9758 - Sensitivity: 1.2216 - precision: 0.3487 - recall: 0.969 - ETA: 28s - loss: -0.2630 - dice_coef: 0.2803 - acc: 0.7606 - mean_squared_error: 0.1050 - Jaccard_index: 0 - ETA: 10s - loss: -0.2640 - dice_coef: 0.2814 - acc: 0.7608 - mean_squared_error: 0.1043 - Jacca\n",
      "Epoch 6/50\n",
      "1632/1632 [==============================] - 385s 236ms/step - loss: -0.3063 - dice_coef: 0.3226 - acc: 0.7777 - mean_squared_error: 0.0823 - Jaccard_index: 0.1934 - Specificity: 0.9758 - Sensitivity: 1.1791 - precision: 0.4044 - recall: 0.9685 - f1score: 0.5675 - val_loss: -0.3432 - val_dice_coef: 0.3592 - val_acc: 0.7813 - val_mean_squared_error: 0.0725 - val_Jaccard_index: 0.2197 - val_Specificity: 0.9671 - val_Sensitivity: 1.1461 - val_precision: 0.4599 - val_recall: 0.8293 - val_f1score: 0.5873- Specificity: 0.9758 - Sensitivity: 1.1840 - precision: - ETA: 1:13 - loss: -0.3009 - dice_coef: 0.3174 - acc: 0.7763 - mean_squared_error: 0.0 - ETA: 28s - loss: -0.3041 - dice_coef: 0.3205 - acc: 0.7777 - mean_squared_error: 0.0830 - Jaccard_index - ETA: 9s - loss: -0.3056 - dice_coef: 0.3219 - acc: 0.7775 - mean_squared_error: 0.0826 - Jaccard_index: 0.1929 - Specificity: 0.9758 - Sensitivity: 1.1797 - precision: 0.403 - ETA: 6s - loss: -0.3057 - dice_coef: 0.3221 - acc: 0.7777 - mean_squared_error: 0.0825 - Jaccard_index: 0.1930 - Specificity: 0.9758 \n",
      "Epoch 7/50\n",
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.3513 - dice_coef: 0.3669 - acc: 0.7911 - mean_squared_error: 0.0644 - Jaccard_index: 0.2260 - Specificity: 0.9757 - Sensitivity: 1.1451 - precision: 0.4619 - recall: 0.9658 - f1score: 0.6216 - val_loss: -0.3871 - val_dice_coef: 0.4025 - val_acc: 0.7973 - val_mean_squared_error: 0.0531 - val_Jaccard_index: 0.2531 - val_Specificity: 0.9663 - val_Sensitivity: 1.1106 - val_precision: 0.5483 - val_recall: 0.8006 - val_f1score: 0.6468696 - Jaccard_index: 0.2137 - Specificity: 0.9759 - Sensitivity: 1.1553 - precision: - ETA: 3:03 - loss: -0.3353 - dice_coef: 0.3511 - acc: 0.7922 - mean_squared_error: 0.0695 - Jaccard_index: 0.2141 - Specificity: 0.9759 - Sensitivity: 1.1 - ETA: 2: - ETA: 2:21 - loss: -0.3398 - dice_coef: 0.3556 - acc: 0.7919 - mean_squared_error: 0.0683 - Jaccard_index: 0.2174 - Specificity: 0.9758 - Sens - ETA: 2:15 - loss: -0.3406 - dice_coef: 0.3564 - acc: 0.7919 - mean_squared_error: 0.0681 - Jaccard_inde - ETA: 2:04 - loss: -0.3408 - dice_coef: 0.3566 - acc: 0.7918 - mean_squared_error: 0.0679 - Jaccard_index: 0.2182 - Specificity: 0.9758 - Sensitivity: 1 - ETA: 1:59 - loss: -0.3409 - dice_coef: 0.3567 - acc: 0.7918 - mean_squared_error: 0.0678 - Jaccard_index: 0.2182 - Specificity: 0.9758 - Sensitivity: 1.1514 - precision: 0 - ETA: 1:55 - loss: -0.3414 - dice_coef: 0.3571 - acc: 0.7909 - mean_squared_error: 0.0678 - Jaccard_index: 0.2185 - Specificity: 0.9758 - Sensitivity: 1.1 - ETA: 1:49 - loss: -0.3417 - dice_coef: 0.3574 - acc: 0.7906 - mean_squared_error: 0.0677 - Jaccar - ETA: 1:39 - loss: -0.3423 - dice_coef: 0.3580 - acc: 0.7908 - mean_squared_error: 0.0674 - Jaccard_index: 0.2192 - Specificity: 0.9758 - Sensitivity: 1.1505 -  - ETA: 1:34 - loss: -0.3431 - dice_coef: 0.3588 - acc: 0.7916 - mean_squared_error: 0.0672 - Jaccard_index: 0.2198 - Specificity: 0.9757 - Sensitivity: 1.1501  - ETA: 1:29 - loss: -0.3436 - dice_coef: 0.3593 - acc: 0.7916 - mean_squared_error: 0.0671 - Jaccard_index: 0.2202 - Specificity: 0.9757 - Sensit - ETA: 1:23 - loss: -0.3437 - dice_coef: 0.3594 - acc: 0.7916 - mean_squared_error: 0.0669 - Jaccard_index: 0.2203 - Specificity: 0.9758 - Sensitivity: 1.1496 - precision: 0.4517  - ETA: 1:20 - loss: -0.3441 - dice_coef: 0.3599 - acc: 0.7915 - mea - ETA: 1:06 - loss: -0.3455 - dice_coef: 0.3612 - acc: 0 - ETA: 44s - loss: -0.3477 - dice_coef: 0.3634 - acc: 0.7919 - mean_squared_error: 0.0656 - Jaccard_index: 0.2233 - Specificity: 0.9757 - Sensitivity: 1.1474 - precision: 0.4571 - recall: 0.9661 - f1score: 0.6 - ETA: 43s - loss: -0.3478 - dice_coef: 0.3635 - acc: 0.7918 - mean_squared_error: 0.0656 - Jaccard_index: 0.2234 - ETA: 25s - loss: -0.3494 - dice_coef: 0.3651 - acc: 0.7916 - mean_squared_error: 0.0651 - Jaccard_index: 0.2246 - Specificity: 0.9757 - Sensitivity: 1.1464 - ETA: 15s - loss: -0.3501 - dice_coef: 0.3658 - acc: 0.7912 - mean_squared_error: 0.0649 - Jaccard_index: 0.2252 - Specificity: 0.9757 - Sensitivit - ETA: 6s - loss: -0.3507 - dice_coef: 0.3663 - acc: 0.7913 - mean_squared_error: 0.0646 - Jaccard_index: 0.2255 - Specificity: 0.9757 - \n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 397s 243ms/step - loss: -0.4006 - dice_coef: 0.4157 - acc: 0.8023 - mean_squared_error: 0.0497 - Jaccard_index: 0.2640 - Specificity: 0.9756 - Sensitivity: 1.1159 - precision: 0.5246 - recall: 0.9627 - f1score: 0.6759 - val_loss: -0.4268 - val_dice_coef: 0.4417 - val_acc: 0.8025 - val_mean_squared_error: 0.0461 - val_Jaccard_index: 0.2846 - val_Specificity: 0.9658 - val_Sensitivity: 1.0883 - val_precision: 0.5857 - val_recall: 0.7898 - val_f1score: 0.6685ef: 0.3912 - acc: 0.7876 - mean_squared_error: 0.0554 - Jaccard_index: 0.2444 - Specificity: 0.9757 - Sensitivity: 1.1286 - precis - ETA: 4:14 - loss: -0.3723 - dice_coef: 0.3877 - acc: 0.7948 - mean_squared_error: 0.0555 - Jaccard_index: 0.2417 - Specificity: 0.9761 - Sensit - ETA: 4:13 - loss: -0.3738 - dice_coef: 0.3891 - acc: 0.7984 - mean_squared_error: 0.0553 - Jaccard_index: 0.2429 - Specificity: 0.9760 - Sensitivity: 1.1287 - precision: 0.4952 - recall: 0.9653 - f1score:  - ETA: 4:12 - loss: -0.3750 - dice_coef: 0.3904 - acc: 0.7974 - mean_squared_error: 0.0553 - Jaccard_index: 0.2439 - Specificity: 0.9759 - Sensitivity: 1.1285 - precision: - ETA: 4:10 - los - ETA: 2:45 - loss: -0.3858 - dice_coef: 0.4011 - acc: 0.8020 - mean_squared_error: 0.0532 - Jaccard_index: 0.2523 - Specificity: 0.9757 - Sensitivity: 1.1235 - precision: 0.5072 - recall: 0.9639 - ETA: 2:44 - loss: -0.3857 - dice_coef: 0.4009 - acc: 0.8018 - mean_squared_error: 0.0531 - Jaccard_index: 0.2522 - Spec - ETA: 2:36 - loss: -0.3870 - dice_coef: 0.4023 - acc: 0.8018 - mean_squared_error: 0.0 - ETA: 2:06 - loss: -0.3901 - dice_coef: 0.4054 - acc: 0.8013 - mean_squared_error: 0.0523 - Jaccard_index: 0.2557 - Specificity: 0.9756 - Sensitivity: 1.1215 - precision: 0.5116 - recall: 0.9633 - f1sco - ETA: 2:05 - loss: -0.3904 - dice_coef: 0.4056 - acc: 0.8012 - mean_squared_error: 0.0523 - Jaccard_index: 0.2559 - Specificity: 0.9755 - Sensitivity: 1.1215 - pr - ETA: 2:01 - loss: -0.3907 - dice_coef: 0.4059 - acc: 0.8012 - mean_squared_error: 0.0522 - Jaccard_index: 0.2561 - Specificity: 0.9755 - Sensitivity: 1.1213 - precision: 0.5124 - re - ETA: 1:58 - loss: -0.3910 - dice_coef: 0.4062 - acc: 0.8009 - mean_squared_error: 0.0522 - Jaccard_index: 0.2563 - Specificity: 0.9755 - Sensitivity: 1.1211 - pr - ETA: 1:53 - loss: -0.3910 - dice_coef: 0.4062 - acc: 0.8011 - mean_squared_error: 0.0520 - Jaccard_index: 0.2563 - Specificity: 0.9756 - Sensitivity: 1.1210 - precision: 0.5131 - recall: 0. - ETA: 1:51 - loss: -0.3912 - dice_coef: 0.4064 - acc: 0.8013 - mean_squared_error: 0.0520 - Jaccard_index: 0.2565 - Specif - ETA: 1:43 - loss: -0.3920 - dice_coef: 0.4072 - acc: 0.8022 - mean_squared_error: 0.0518 - Jaccard_index: 0.2571 - Specificity: 0.9756 - Sensitivity: 1.1205 - precision: 0.5142 - recall: 0.96 - ETA: 1:42 - loss: -0. - ETA: 1:24 - loss: -0.3939 - dice_coef: 0.4091 - acc: 0.8031 - mean_squared_error: 0.0514 - Jaccard_index: 0.2586 - Specific - ETA: 1:16 - loss: -0.3945 - dice_coef: 0.4097 - acc: 0.8037 - mean_squared_error: 0.0512 - Jaccard_index: 0.2591 - Specificity: 0.9756 - Sensitivity: 1.1193 - precision: 0.5171 - reca - ETA: 1:14 - loss: -0.3947 - dice_coef: 0.4099 - acc: 0.8036 - mean_squared_error: 0.0511 - Jaccard_index: 0.2593 - Specificity: 0.9756 - Sensitivity: 1.1192 - precision: 0.5175 - recall: 0.9632 - f1score - ETA: 1:13 - loss: -0.3948 - dice_coef: 0.4100 - acc: 0.8037 - mean_squared_error: 0.0511 - Jaccard_index: 0.2594 - Specificit - ETA: 1:05 - loss: -0.3955 - dice_coef: 0.4107 - acc: 0.8031 - mean_squared_error: 0.0510 - Jaccard_index: 0. - ETA: 53s - loss: -0.3959 - ETA: 19s - loss: -0.3991 - dice_coef: 0.4143 - acc: 0.8023 - mean_squared_error: 0.0500 - Jaccard_index: 0.2629 - Specificity: 0.9756 - Sensitivity: 1.1167 - precision: 0.5230 - recall: 0.9629 - f1sco - ETA: 17s - loss: -0.3992 - dice_coef: 0.4143 - acc: 0.8021 - mean_squared_error: 0.0500 - Jaccard_index: 0.2629 - Specificity: 0.9756 - Sensitivity: 1.1167 - precision: - ETA: 9s - loss: -0.3999 - dice_coef: 0.4150 - acc: 0.8023 - mean_squared_error: 0.0498 - Jaccard_index\n",
      "Epoch 9/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.4532 - dice_coef: 0.4679 - acc: 0.8115 - mean_squared_error: 0.0380 - Jaccard_index: 0.3075 - Specificity: 0.9754 - Sensitivity: 1.0914 - precision: 0.5902 - recall: 0.9580 - f1score: 0.7274 - val_loss: -0.4687 - val_dice_coef: 0.4832 - val_acc: 0.8083 - val_mean_squared_error: 0.0387 - val_Jaccard_index: 0.3201 - val_Specificity: 0.9655 - val_Sensitivity: 1.0690 - val_precision: 0.6375 - val_recall: 0.7838 - val_f1score: 0.6989ef: 0.4562 - acc: 0.8072 - mean_squared_error: 0.0402 - Jaccard_index: 0.2974 - Specificit - ETA: 2:03 - loss: -0.4423 - dic - ETA: 39s - loss: -0.4491 - dice_coef: 0.4638 - acc: 0.8101 - mean_squared_error: 0.0386 - Jaccard_index: 0.3039 - Specificity: 0.9755 - Sensitivity: 1\n",
      "Epoch 10/50\n",
      "1632/1632 [==============================] - 398s 244ms/step - loss: -0.5080 - dice_coef: 0.5223 - acc: 0.8187 - mean_squared_error: 0.0288 - Jaccard_index: 0.3559 - Specificity: 0.9753 - Sensitivity: 1.0709 - precision: 0.6556 - recall: 0.9529 - f1score: 0.7740 - val_loss: -0.4822 - val_dice_coef: 0.4964 - val_acc: 0.7980 - val_mean_squared_error: 0.0489 - val_Jaccard_index: 0.3318 - val_Specificity: 0.9666 - val_Sensitivity: 1.0730 - val_precision: 0.5599 - val_recall: 0.8327 - val_f1score: 0.6649- Jaccard_index: 0.3357 - Specificity: 0.9753 - Sensitivity: 1.0789 - precision: 0.6317 - recall: 0.9570 - - ETA: 4:07 - loss: -0.4863 - dice_coef: 0.5008 - acc: 0.8166 - mean_squared_error: 0.0318 - Jaccard_index: 0.3360 - Sp - ETA: 4:01 - loss: -0.4865 - dice_coef: 0.5010 - acc: 0.8193 - ETA: 3:51 - loss: -0.4872 - dice_coef: 0.5017 - acc: 0.8193 - mean_squared_error: 0.0316 - Jaccard_index: 0.3369 - Specificity: 0.9754 - Sensitivity: 1.0783 - precision: 0.6 - ETA: 3:48 - loss: -0.4862 - dice_coef: 0.5007 - acc: 0.8191 - mean_squared_error: 0.0317 - Jaccard_index: 0.3360 - Specificity: 0.9755 - Sensitivity: 1.0783 - prec - ETA: 3:44 - loss: -0.4871 - dice_coef: 0.5016 - acc: 0.8190 - mean_squared_error: - ETA: 3:34 - loss: -0.4865 - dice_coef: 0.5010 - acc:  - ETA: 1:47 - loss: -0.4987 - dice_coef: 0.5131 - acc: 0.8192 - mean_squared_error: 0.0300 - Jaccard_index: 0.3474 - Specificity: 0.9754 - Sensitivity: 1.0741 - precision: 0.6460  - ETA: 1:44 - loss: -0.4990 - dic - ETA: 4s - loss: -0.5074 - dice_coef: 0.5218 - acc: 0.8193 - mean_squared_error: 0.0289 - Jaccard_index: 0.3554 - Specificity: 0.9753 - Sensitivity: 1.0711 - precision: 0.6549 - re - ETA: 1s - loss: -0.5077 - dice_coef: 0.5220 - acc: 0.8190 - mean_squared_error: 0.0288 - Jaccard_index: 0.3557 - Specificity: 0.9753 - Sensitivity: 1.0710 - precision: 0.6554 - recall: 0.95\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 398s 244ms/step - loss: -0.5622 - dice_coef: 0.5763 - acc: 0.8238 - mean_squared_error: 0.0225 - Jaccard_index: 0.4073 - Specificity: 0.9751 - Sensitivity: 1.0545 - precision: 0.7123 - recall: 0.9472 - f1score: 0.8108 - val_loss: -0.5451 - val_dice_coef: 0.5590 - val_acc: 0.8142 - val_mean_squared_error: 0.0315 - val_Jaccard_index: 0.3899 - val_Specificity: 0.9650 - val_Sensitivity: 1.0423 - val_precision: 0.7027 - val_recall: 0.7722 - val_f1score: 0.7317ecision: 0.6976  - ETA: 2:53 - loss: -0.5471 - dice_coef: 0.5613 - acc: 0.8231 - mea - ETA: 2:21 - loss: -0.5482 - dice_coef: 0.5623 - acc: 0.8243 - mean_squared_error: 0.0238 - Jaccard_index: 0.3935 - Specificity: 0.9753 - Sensitivity: 1.0582 - precision: 0.6979 - recall: 0.9486 - f1score:  - ETA: 2:21 - loss: -0.5480 - - ETA: 1:45 - loss: -0.5523 - dice_coef: 0.5664 - acc: 0.8237 - mean_squared_error: 0.0235 - Jaccard_index: 0.3975 - Specificity: 0.9752 - Sensitivity: 1.0572 - precision: 0.7020  - ETA: 1:43 - loss: -0.5528 - dice_coef: 0.5670 - acc: 0.8231 - mean_squared_error: 0.0234 - Jaccard_index: 0.3981 - Specificity: 0.9752 - Sensitivity: 1.0571 - precision: 0.7028 - recall: 0.94 - ETA: 1:41 - loss: -0.5530 - dice_coef: 0.5671 - acc: 0.8233 - mean_squared_error: 0.0234 - Jaccard_index: 0.3982 - Specificity: 0.9752 - Sensitivity: 1.0571 - precision: 0.7028 - recall: 0.9480 - f - ETA: 1:39 - loss: -0.5531 - dice_coef: 0.5672 - acc: 0.8231 - mean_squared_error: 0.0234 - Jaccard_index: - ETA: 1:10 - loss: -0.5552 - dice_coef: 0.5693 - acc: 0.8224 - mean_squared_error: 0.0232 - Jaccard_index: 0.4004 - Specificity: 0.9752 - Sensitivity: 1.0563 - precision: 0.7045 - recall: 0. - ETA: 1:08 - loss: -0.5554 - dice_coef: 0.5695 - acc: 0.8222 - mean_squared_error: 0.0232 - Jaccard_index: 0.4006 - Specificity: 0.9752 - Sensitivity: 1.0563 - precision: 0.7049 - recall: 0.94 - ETA: 1:07 - loss: -0.5555 - dice_coef: 0.5696 - acc: 0.8222 - mean_squared_error: 0.0231 - Jaccard_index: 0.4007 - Specificity: 0.9752 - Sensitivity: 1.0563 - prec - ETA: 1\n",
      "Epoch 12/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.6136 - dice_coef: 0.6275 - acc: 0.8276 - mean_squared_error: 0.0179 - Jaccard_index: 0.4597 - Specificity: 0.9750 - Sensitivity: 1.0416 - precision: 0.7613 - recall: 0.9411 - f1score: 0.8399 - val_loss: -0.5749 - val_dice_coef: 0.5886 - val_acc: 0.8138 - val_mean_squared_error: 0.0318 - val_Jaccard_index: 0.4190 - val_Specificity: 0.9652 - val_Sensitivity: 1.0366 - val_precision: 0.6954 - val_recall: 0.7855 - val_f1score: 0.7337459 - precision: 0.7452 - recall: 0.9440 - f1score: 0. - ETA: 3:29 - loss: -0.5951 - dice_coef: 0.6091 - acc: 0.8216 - mean_squared_error: 0.0192 - Jaccard_index: 0.4402 - Specificity: 0.9752 - Sensitivity: 1.0459 - precision: 0.744 - ETA: 3:26 - loss: -0.5947 - dice_coef: 0.6086 - acc: 0.8228 - mean_squared_erro - ETA: 2:01 - loss: -0.6020 - dice_coef: 0.6159 - acc: 0.8259 - mean_squared_error: 0.0186 - Jaccard_index: 0.4475 - Specificit - ETA: 1:54 - loss: -0.6031 - dice_coef: 0.6170 - acc: 0.8262 - mean_squared_error: 0.0186 - Jaccard_index: 0.4487 - Specificity: 0.9752 - Sensitivity: 1.0439 - precision: 0.7 - ETA: 1:50 - loss: -0.6036 - dice_coef: 0.6175 - acc: 0.8260 - mean_squared_error: 0.0186 - Jaccard_in - ETA - ETA: 1:22 - loss: -0.6069 - dice_coef: 0.6207 - acc: 0.8265 - mean_ - ETA: 1:09 - loss: -0.6076 - dice_coef: 0.6215 - acc: 0.8267 - mean_squared_error: 0.0183 - - ETA: 57s - loss: -0.6089 - dice_coef: 0.6228 - acc: 0.8273 - mean_squared_error: 0.0182 - Jaccard_index: 0.4548 - Specificity: 0.9750 - Sensitivity: 1.0427 - precision:  - ETA: 50s - loss: -0.6093 - dice_coef: 0.6232 - acc: 0.8282 - mean_squared_error: 0.0182 - Jaccard_index: 0.4552 - Specificity: 0.9751 - Sensitivity: 1.0426 - precision: 0.7578 - recall: 0.9423 - f1score: 0 - ETA: 49s - loss: -0.6093 - dice_coef: 0.6232 - acc: 0.8282 - mean_squared_error: 0.0182 - Jaccard_index: 0.4552 - Specificity:  - ETA: 34s - loss: -0.6105 - dice_coef: 0.6244 - acc: 0.8269 - mean_squared_error: 0.0181 - Jaccard_index: 0.4564 - Specificity: 0.9751 - Sensitivity: 1.0423 - precision: 0.7590 - recall: 0.9420 - f1scor - ETA: 33s - loss: -0.6106 - dice_coef: 0.6245 - acc: 0.8269 - mean_squared_error: 0.0181 - Jaccard_index: 0.4565 - Spe - ETA: 16s - loss: -0.6123 - dice_coef: 0.6262 - acc: 0.8268 - mean_squared_erro - ETA: 1s - loss: -0.6135 - dice_coef: 0.6274 - acc: 0.8274 - mean_squared_error: 0.0179 - Jaccard_index: 0.4596 - Specificity: 0.9750 - Sensitivity: 1.0416 - precision: 0.7613 - recall: 0.9412 - f\n",
      "Epoch 13/50\n",
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.6589 - dice_coef: 0.6725 - acc: 0.8302 - mean_squared_error: 0.0149 - Jaccard_index: 0.5090 - Specificity: 0.9748 - Sensitivity: 1.0317 - precision: 0.8003 - recall: 0.9346 - f1score: 0.8607 - val_loss: -0.6188 - val_dice_coef: 0.6324 - val_acc: 0.8202 - val_mean_squared_error: 0.0249 - val_Jaccard_index: 0.4647 - val_Specificity: 0.9644 - val_Sensitivity: 1.0224 - val_precision: 0.7814 - val_recall: 0.7578 - val_f1score: 0.7663squared_error: 0.0162 - Jaccard_index: 0.4823 - Specificity: 0.9751 - Sensitivity: 1.0359  - ETA: 4:01 - loss: -0.6382 - dice_coef: 0.6519 - acc: 0.8211 - mean_squared_error: 0.0161 - Jaccard_index: 0.4860 - Specificity: 0.9749 - Sensitivity: 1.0356 - precision: 0.7849 -  - ETA: 3:59 - loss: -0.6402 - dice_coef: 0.6539 - acc: 0.8230 - mean_squared_error: 0.0161 - Jaccard_index: 0 - ETA: 2:24 - loss: -0.6492 - dice_coef: 0.6629 - acc: 0.8278 - mean_squared_error: 0.0155 - Jaccard_index: 0.4981 -  - ETA: 2:16 - loss: -0.6499 - dice_coef: 0.6636 - acc: 0.8285 - mean_squared_error: 0.0155 - Jaccard_index: 0.4988 - Specificity: 0.9748 - Sensitivity: 1.0336 - precision: 0.7927 - recall: 0.9356 - - ETA: 2:14 - loss: -0.6500 - dice_coef: 0.6637 - acc: 0.8285 - mean_squared_error: 0.0155 - Jaccard_index: 0.4990 - Specificity: 0.9748 - Sensitivity: 1.0336 - precision: 0 - ETA: 2:11 - - ETA: 1:33 - loss: -0.6528 - dice_coef: 0.6665 - acc: 0.8292 - mean_squared_error: 0.0153 - Jaccard_index: 0.5021 - Specificity: 0.9748 -  - ETA: 1:26 - loss: -0.6532 - dice_coef: 0.6669 - acc: 0.8287 - mean_squared_error: 0.0152 - Jaccard_index: 0.5026 - Specificity: 0.9748 - Sensitivity: 1.0 - ETA: 1:21 - loss: -0.6538 - dice_coef: 0.6675 - acc: 0.8293 - mean_squared_error: 0.0152 - Jaccard_ - ETA: 1:10 - loss: -0.6547 - dice_coef: 0.6683 -  - ETA: 52s - loss: -0.6554 - dice_coef: 0.6690 - acc: 0.8294 - mean_squared_error: 0.0151 - Jaccard_index: 0.5050 - Specificity: 0.9748 - Sensitiv - ETA: 40s - loss: -0.6559 - dice_coef: 0.6695 - acc: 0.8295 - mean_squared_error: 0 - ETA: 16s - loss: -0.6572 - dice_coef: 0.6709 - acc: 0.8301 - mean_squared_error: 0.0150 - Jaccard_index: 0.5072 - Specificity: 0.9749 - Sensitivity: 1.0320 - precision: 0.7987 - reca - ETA: 11s - loss: -0.6576 - dice_coef: 0.6712 - acc: 0.8300 - mean_squared_error: 0.0149 - Ja\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 399s 244ms/step - loss: -0.6979 - dice_coef: 0.7113 - acc: 0.8320 - mean_squared_error: 0.0128 - Jaccard_index: 0.5542 - Specificity: 0.9747 - Sensitivity: 1.0242 - precision: 0.8304 - recall: 0.9291 - f1score: 0.8757 - val_loss: -0.6370 - val_dice_coef: 0.6504 - val_acc: 0.8227 - val_mean_squared_error: 0.0233 - val_Jaccard_index: 0.4848 - val_Specificity: 0.9634 - val_Sensitivity: 1.0139 - val_precision: 0.8226 - val_recall: 0.7244 - val_f1score: 0.7663ivity: 1.0270 - precision: 0.8175 - recall: 0.93 - ETA: 3:37 - loss: -0.6815 - dice_coef: 0.6950 - acc: 0.8324 - mean_squared_error: - ETA: 3:27 - loss: -0.6837 - dice_coef: 0. - ETA: 3:12 - loss: -0.6851 - dice_coef: 0.6986 - acc: 0.8337 - mean_squared_error: 0.0133 - Jaccard_in - ETA: 3:02 - loss: -0.6858 - dice_coef: 0.6993 - acc: 0.8328 - mean_squared_error: 0 - ETA: 2:51 - loss: -0.6877 - dice_coef: 0.7012 - acc: 0.8334 - mean_squared_error: 0.0132 - Jaccard_index: 0.5421 - Specificity: 0.9748 - Sensitivity: 1.026 - ETA: 2:46 - loss: -0.6881 - dice_coef: 0.7016 - acc: 0.8328 - mean_squared_error: 0.0132 - Jaccard_index: 0.5426 - Specificity: 0.9748 - Sensitivity: 1.0261 - precision: 0.8230 - recall: 0.9315 - f1s - ETA: 2:45 - loss: -0.6880 - dice_coef: 0.7015 - acc: 0.8330 - mean_squared_error: 0.0132 - Jaccard_index: 0.5425  - ETA: 2:37 - loss: -0.6889 - dice_coef: 0.7024 - acc: 0.8311 - mean_squared_error: 0.0132 - Jaccard_index: 0.5435 - Specificity: 0.9748 - Sensit - ETA: 2:31 - loss: -0.6891 - dice_coef: 0.7026 - acc: 0.8318 - mean_squared_error: 0.0 - ETA: 2:20 - loss: -0.6901 - dice_coef: 0.7036 - acc: 0.8304 - mean_squared_error: 0.0131 - Jaccard_index: 0.5449 - Specificity: 0.9747 - Sensitivity: 1.0257 - precision: 0.8248 - recall: 0.93 - ETA: 2:18 - loss: -0.6899 - dice_coef: 0.7034 - acc: 0.8300 - mean_squared_error: 0.0132 - Jaccard_index: 0.5447 - Specificity: 0.9748 - Sensitivity: 1.0257 - precision: 0.8244 - recall: 0.9308 - f1sco - ETA: 2:17 - loss: -0.6901 - dice_coef: 0.7036 - acc: 0.8302 - mean_squared_error: - ETA: 2:06 - loss: -0.6906 - dice_coef: 0.7040 - acc: 0.8314 - mean_squared_error: 0.0131 - Jaccard_index: 0.5454 - Specificity: 0.9748 - Sensitivity: 1.0256 - precision: 0.824 - ETA: 2:03 - loss: - - ETA: 1:26 - loss: -0.6935 - dice_coef: 0.7069 - acc: 0.8306 - mean_squared_error: 0.0130 - Jacca - ETA: 54s - loss: -0.6957 - dice_coef: 0.7092 - acc: 0.8309 - mean_squared_error: 0.0129 - Jaccard_index: 0.5516 - Specificity: 0.9747 - Sensitivity: 1.0248 - precision: 0.8288 - recall: 0.9300 - f1score:  - ETA: 53s - loss: -0.6958 - dice_coef: 0.7093 - acc: 0.8309 - mean_squared_error: 0.0129 - Jaccard_index: 0.5517 - Specificity: 0. - ETA: 38s - loss: -0.6964 - dice_coef: 0.7099 - acc: 0.8307 - mean_squared_error: 0.0129 - Jaccard_index: 0.5525 - Specificity: 0.9746 - Sensitivity: 1.0246 - precision: 0.8293 - recal - ETA: 33s - loss: -0.6965 - dice_coef: 0.7100 - acc: 0.8310 - mean_squared_error: 0.0129 - J - ETA: 12s - loss: -0.6977 - dice_coef: 0.7112 - acc: 0.8313 - mean_squared_error: 0.0129 \n",
      "Epoch 15/50\n",
      "1632/1632 [==============================] - 401s 245ms/step - loss: -0.7313 - dice_coef: 0.7445 - acc: 0.8334 - mean_squared_error: 0.0113 - Jaccard_index: 0.5949 - Specificity: 0.9746 - Sensitivity: 1.0186 - precision: 0.8555 - recall: 0.9246 - f1score: 0.8877 - val_loss: -0.6391 - val_dice_coef: 0.6523 - val_acc: 0.8239 - val_mean_squared_error: 0.0232 - val_Jaccard_index: 0.4872 - val_Specificity: 0.9620 - val_Sensitivity: 1.0065 - val_precision: 0.8597 - val_recall: 0.6757 - val_f1score: 0.7521851 -  - ETA: 2:58 - loss: -0.7243 - dice_coef: 0.7376 - - ETA: 1:47 - loss: -0.7263 - dice_coef: 0.7396 - acc: 0.8327 - mean_squared_error: 0.0 - ETA: 1:35 - loss: -0.7269 - dice_coef: 0.7402 - acc: 0.8329 - mean_squared_error: 0.0114 - Jaccard_index: 0.5894 - Specificity: 0.9747 - Sensitivity: 1.0194  - ETA: 1:31 - loss: -0.7275 - dice_coef: 0.7408 - ac - ETA: 54s - loss: -0.7292 - dice_coef: 0.7424 - acc: 0.8330 - mean_squared_error: 0.0114 - Jaccard_index: 0.5923 - Specificity: 0.9745 - Sensitivity: 1.0190 - precision: 0.8540 - recall: 0.9251 - f1score: 0.88 - ETA: 54s - loss: -0.7291 - dice_coef: 0.7424 - acc: 0.8331 - mean_squared_error: 0.0114 - Jaccard_index: 0.5922 - Specificity: 0.9746 - Sensitivity: 1.0190 - precision: 0.8540 - recall: 0.9251 - - ETA: 51s - loss: -0.7292 - dice_coef: 0.7425 - acc: 0.8331 - mean_squared_error: 0.0114 - Jaccard_index: 0.5924 - Specificity: 0.9746 - Sensitivity - ETA: 39s - loss: -0.7300 - dice_coef: 0.7432 - acc: 0.8325 - mean_squared_error: 0.0114 - Jaccard_index: 0.5933 - Specificity: 0.9746 - Sensitivity: 1.0189 - precision: 0.8546 - recall: 0.9251 - f1score: 0. - ETA: 38s - loss: -0.7300 - dice_coef: 0.7432 - acc: 0.8327 - mean_squared_error: 0.0114 - Jaccard_index: 0.5933 - Specificity: 0.9746 - Sensitivity: 1.0189 - precision: 0.8546 - recall: 0.9251 - f - ETA: 36s - loss: -0.7300 - dice_coef: 0.7433 - acc: 0.8328 - mean_squared_error: 0.0114 - Jaccard_index: 0.5933 - Specificity: 0.9746 - Sensitivity: 1.0189 - precision: 0.8547 - recall: 0.9250 - f1score:  - ETA: 35s - loss: -0.7300 - dice_coef: 0.7432 - acc: 0.8328 - mean_squared_error: 0.0114 - Jaccard_inde - ETA: 15s - loss: -0.7306 - dice_coef: 0.7438 - acc: 0.8332 - mean_squared_error: 0.0113 - Jaccard_index: 0.5940 - Specificity: 0.9746 - Sensitivity: 1.0187 - precision: 0.8549 - recall: 0.925 - ETA: 12s - loss: -0.7308 - dice_coef: 0.7440 - acc: 0.8331 - mean_squared_error: 0.0113 - Jaccard_index: 0.5943 - Specificity: 0.9746 - Sensitivity: 1.0187 - precision: 0.8550 - recall: 0.9248 - f1score: 0.887 - ETA: 11s - loss: -0.7308 - dice_coef: 0.7441 - acc: 0.8331 - mean_squared_error: 0.0113 - J\n",
      "Epoch 16/50\n",
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.7584 - dice_coef: 0.7715 - acc: 0.8344 - mean_squared_error: 0.0103 - Jaccard_index: 0.6296 - Specificity: 0.9745 - Sensitivity: 1.0145 - precision: 0.8745 - recall: 0.9214 - f1score: 0.8965 - val_loss: -0.6738 - val_dice_coef: 0.6868 - val_acc: 0.8249 - val_mean_squared_error: 0.0213 - val_Jaccard_index: 0.5267 - val_Specificity: 0.9629 - val_Sensitivity: 1.0050 - val_precision: 0.8725 - val_recall: 0.7066 - val_f1score: 0.7767 0.0103 - Jaccard_index: 0.6 - ETA: 1:12 - loss: -0.7553 - dic - ETA: 52s - loss: -0.7566 - dice_coef: 0.7697 - acc: 0.8335 - mean_squared_error: 0.0103 - Jaccard_index: 0.6273 - Specificity: 0.9745 - Sensitivity: 1.0147 - - ETA: 42s - loss: -0.7569 - dice_coef: 0.7700 - acc: 0.8338 - mean_squared_error: 0.0103 - Jaccard_index: 0.6277 - Specificity: 0.9745 - Sensitivity: 1.0147 - precision: 0.8734  - ETA: 36s - loss: -0.7575 - dice_coef: 0.7706 - acc: 0.8345 - mean_squared_error: 0.0103 - Jaccard_index: 0.6285 - Specificity: 0.9744 - Sensitivity: 1.0146 - precision:\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 407s 250ms/step - loss: -0.7799 - dice_coef: 0.7928 - acc: 0.8351 - mean_squared_error: 0.0095 - Jaccard_index: 0.6582 - Specificity: 0.9745 - Sensitivity: 1.0114 - precision: 0.8899 - recall: 0.9193 - f1score: 0.9036 - val_loss: -0.6851 - val_dice_coef: 0.6980 - val_acc: 0.8254 - val_mean_squared_error: 0.0211 - val_Jaccard_index: 0.5400 - val_Specificity: 0.9627 - val_Sensitivity: 1.0024 - val_precision: 0.8827 - val_recall: 0.6999 - val_f1score: 0.7764ror: 0.0095 - Jaccard_index: 0.6474 - Specificity: 0.9750 - Sensitiv - ETA: 3:35 - loss: -0.7727 - dice_coef: 0.7857 - acc: 0.8394 - mean_squared_error: 0.0095 - Jaccard_index: 0.6485 - Specific - ETA: 3:10 - loss: -0.7751 - dice_coef: 0.7880 - acc: 0.8385 - mean_squared_error: 0.0095 - Jaccard_index: 0.6517 - Specificity: 0.9747 - Sensitivity: 1 - ETA: 3:05 - loss: -0.7754 - dice_coef: 0.7883 - acc: 0.8387 - mean_squared_erro - ETA: 2:54 - l - ETA: 2:17 - loss: -0.7763 - dice_coef: 0.7892 - acc: 0.8384 - mean_squared_error: 0.0095 - Jaccard_index: 0.6533 - Specificity: 0.9746 - Sens - ETA: 2:11 - loss: -0.7763 - dice_coef: 0.7893 - acc: 0.83 - ETA: 1:57 - loss: -0.7770 - dice_coef: 0.7899 - acc: 0.8372 - mean_squared_error: 0.0095 - Jaccard_index: 0.6543 - Specificity: 0.9745  - ETA: 1:50 - loss: -0.7769 - dice_coef: 0.7899 - acc: 0.8368 - mean_squared_error: 0.0095 - Jaccard_index: 0.6542 - Specificity: 0.9745 - Sensitivity: 1.0118 - precision: 0.888 - ETA: 1:46 - loss: -0.7770 - dice_coef: 0.7899 - acc: 0.8365 - mean_squared_error: 0.0095 - Jaccard_index: 0.6543 - Specificity: 0.9745 - Sensitivity: 1.0118 - precision: 0.8885 - recall:  - ETA: 1:44 - loss: -0.7770 - dice_coef: 0.7900 - acc: 0.8367 - mean_squared_error: 0.0095 - Jaccar - ETA: 1:34 - loss: -0.7776 - dice_coef: 0.7906 - acc: 0.8370 - mean_squa - ETA: 1:21 - loss: -0.7782 - dice_coef: 0.7911 - acc: 0.8373 - mean_squared_error: 0.0095 - Jac - ETA: 42s - loss: -0.7791 - dice_coef: 0.7920 - acc: 0.8366 - mean_squared_error: 0.0095 - Jaccard_index: 0.6571 - Specificity: 0.9745 - Sensitivity: 1.0115 - precision - ETA: 34s - loss: -0.7792 - dice_coef: 0.7921 - acc: 0.8366 - mean_squared_error: 0.0095 - Jaccard_index: 0.6572 - Specificity: 0.9745 - Sensiti - ETA: 22s - loss: -0.7792 - dice_coef: 0.7922 - acc: 0.8358 - mean_squared_error: 0.0095 - Jaccard_index: 0.6573 - Specificity: 0.9745 - Sensitivity: 1.0115 - precision: 0.8893 - recall: 0.9193 - f1score - ETA: 21s - loss: -0.7793 - dice_coef: 0.7922\n",
      "Epoch 18/50\n",
      "1632/1632 [==============================] - 402s 246ms/step - loss: -0.7965 - dice_coef: 0.8093 - acc: 0.8357 - mean_squared_error: 0.0089 - Jaccard_index: 0.6810 - Specificity: 0.9744 - Sensitivity: 1.0091 - precision: 0.9012 - recall: 0.9178 - f1score: 0.9088 - val_loss: -0.6834 - val_dice_coef: 0.6961 - val_acc: 0.8251 - val_mean_squared_error: 0.0219 - val_Jaccard_index: 0.5376 - val_Specificity: 0.9622 - val_Sensitivity: 1.0005 - val_precision: 0.8813 - val_recall: 0.6846 - val_f1score: 0.7661 acc: 0.8249 - mean_squared_error: 0.0089 - Jaccard_index: 0.6776 - Specificity: 0.9737 - Sensitivity: - ETA: 4:00 - loss: -0.7903 - dice_coef: 0.8031 - acc: 0.8388 - mean_squared_error: 0.0089 - Jaccard_index: 0.6724 - Specific - ETA: 4:13 - loss: -0.7907 - dice_coef: 0.8035 - acc: 0.8357 - mean_squared_error: 0.0089 - Jaccard_index: 0.6731 -  - ETA: 4:11 - loss: -0.7867 - dice_coef: 0.7995 - acc: 0.8368 - ETA: 4:07 - loss: -0.7881 - dice_coef: 0.8009 - acc: 0.8350 - mean_squared_error: 0.0090 - Jaccard_index: 0.6695 - Specificity: 0.9746 - Sensitivity: 1.0100 - precision: 0.8 - ETA: 4:04 - loss: -0.7873 - dice_coef: 0.8001 - acc: 0.8339 - mean_squared_error: 0.0090 - Jaccard_in - ETA: 3:56 - loss: -0.7893 - dice_coef: 0.8021 - acc: 0.8332 - mean_squared_error: 0.0090 - Jaccard_index: 0.6712 - Specificity: 0.9745 - Sensitivity: 1.0098 - precision: 0.8982 - reca - ETA: 3:54 - loss: -0.7897 - dice_coef: 0.8025 - acc: 0.8346 - mean_squared_error: 0.0090 - Jaccard_index: 0.6716 - Specificity: 0.9745 - Sensitivity: 1.009 - ETA: 3:50 - loss: -0.7900 - dice_coef: 0.8029 - acc: 0.8339 - mean_squared_error: 0.0091 - Jaccard_index: 0.6721 - Specif - ETA:  - ETA: 1:51 - loss: -0.7940 - dice_coef: 0.8068 - acc: 0.8346 - mean_squared_error: 0.0089 - Jaccard_index: 0.6775 - Specificity: 0.9745 - Sensitivity: 1.0094 - precision: 0.8993 -  - ETA: 1:48 - loss: -0.7941 - dice_coef: 0.8069 - acc: 0.8348 - mean_squared_error: 0.0089 - Jaccard_index: 0.6777 - Specificity: 0.9745 - Sensitivity: 1.0094 - p\n",
      "Epoch 19/50\n",
      "1632/1632 [==============================] - 403s 247ms/step - loss: -0.8100 - dice_coef: 0.8226 - acc: 0.8361 - mean_squared_error: 0.0084 - Jaccard_index: 0.6998 - Specificity: 0.9744 - Sensitivity: 1.0075 - precision: 0.9105 - recall: 0.9177 - f1score: 0.9135 - val_loss: -0.7055 - val_dice_coef: 0.7180 - val_acc: 0.8249 - val_mean_squared_error: 0.0211 - val_Jaccard_index: 0.5631 - val_Specificity: 0.9632 - val_Sensitivity: 1.0019 - val_precision: 0.8709 - val_recall: 0.7226 - val_f1score: 0.78630.8170 - acc: 0.8462 - mean_squared_error: 0.0085 - Jaccard_index: 0.6921 - Specificity: 0.9748 - Sensitivity: 1.0082 - precision: 0.9 - ETA: 4:01 - loss: -0.8038 - dice_coef: 0.8165 - acc: 0.8412 - mean_squared_error: 0.0086 - Jaccard_index: 0.6913 - Specificity: 0.9746 - Sensitivity: 1.0081 - precision: 0.9053 - recall: 0. - ETA: 4:00 - loss: -0.8034 - dice_coef: 0.8160 - acc: 0.8413 - mean_squared_error: 0.0086 - Jaccard_index: - ETA: 3:53 - loss: -0.8040 - dice_coef: 0.8167 - acc: 0.8405 - mean_squared_error: 0.0087 - Jaccard_index: 0.6915 - Specificity: 0.9745 - Sensitivity: 1.0081 - precision: 0.9061 - re - ETA: 3:51 - loss: -0.8044 - dice_co - ETA: 2:59 - loss: -0.8079 - dice_coef: 0.8206 - acc: 0.8425 - mean_squared_error: 0.0085 - Jaccard_index: 0 - ETA: 2:50 - loss: -0.8078 - dice_coef: 0.8204 - acc: 0.8423 - mean_squared_error: 0.0085 - Jaccard_index: 0.6967 - Specificity: 0.9745  - ETA: 2:43 - loss: -0.8078 - dice_coef: 0.8204 - acc: 0.8421 - mean_squared_error: 0.0085 - Jaccard_index: 0.6967 - Specificity: 0.9745 - Sensit - ETA: 2:37 - loss: -0.8075 - dice_coef: 0.8201 - acc: 0.8406 - - ETA: 2:23 - loss: -0.8074 - dice_co - ETA: 2:07 - loss: -0.8077 - dice_coef: 0.8203 - acc: 0.8383 - mean_squared_error: 0.0085 - Jaccard_index: 0.6966 - Specificity: 0.9744 - Sensitivity: 1.0 - ETA: 2:01 - loss: -0.8079 - dice_coef: 0.8205 - acc: 0.8385 - mean_squared_error: 0.0085 - Jaccard_index: 0.6969 - Specificity: 0.9745 - Sensitivity: 1.0078 - precision: 0.9089 - recall: 0.9179 - f - ETA: 1:41 - loss: -0.8079 - dice_coef: 0.8205 - acc: 0.8374 - mean_squared_error: 0.0085 - Jaccard_index: 0.6969 - Specificity: 0.9744 - Sensitivity: 1.007 - ETA: 1:36 - loss: -0.8079 - dice_coef: 0.8205 - acc:  - ETA: 1:22 - loss: -0.8081 - dice_coef: 0.8207 - acc: 0.8368 - mean_squared_error: 0.0085 - Jaccard_index: 0.6972 - Specificity: 0.9744 - Sensitivity: 1.0077 -  - ETA: 1:17 - loss: -0.8081 - dice_coef: 0.8207 - acc: 0.8368 - mean_squared_error: 0.0085 - Jaccard_index: 0.6971 - Specificity: 0.9744 - Sensitivity: 1.0077 - precision: 0.9092 - recall: 0.9175 - f1score:  - ETA: 1:16 - loss: -0.8082 - dice_coef: 0.8208 - acc: 0.8370 - mean_squared_error: 0.0085 - Jaccard_index: - ETA: 35s - loss: -0.8092 - dice_coef: 0.8 - ETA: 6s - loss: -0.8099 - dice_coef: 0.8225 - acc: 0.8366 - mean_squared_error: 0.0084 - Jaccard_index: 0.6997 - Specificity: 0.9744 - Sensit - ETA: 0s - loss: -0.8100 - dice_coef: 0.8226 - acc: 0.8362 - mean_squared_error: 0.0084 - Jaccard_index: 0.6998 - Specificity: 0.9744 - Sensitivity: 1.0075 - precision: 0.9105 - recall: 0.9177 - f1score:  - ETA: 0s - loss: -0.8100 - dice_coef: 0.8226 - acc: 0.8361 - mean_squared_error: 0.0084 - Jaccard_index: 0.6998 - Specificity: 0.9744 - Sensitivity: 1.0075 - precision: 0.9105 - recall: 0.9177 - f1score: 0.91\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 396s 242ms/step - loss: -0.8203 - dice_coef: 0.8327 - acc: 0.8364 - mean_squared_error: 0.0081 - Jaccard_index: 0.7145 - Specificity: 0.9744 - Sensitivity: 1.0063 - precision: 0.9175 - recall: 0.9180 - f1score: 0.9173 - val_loss: -0.6751 - val_dice_coef: 0.6874 - val_acc: 0.8257 - val_mean_squared_error: 0.0226 - val_Jaccard_index: 0.5284 - val_Specificity: 0.9609 - val_Sensitivity: 0.9959 - val_precision: 0.9076 - val_recall: 0.6398 - val_f1score: 0.7454 - loss: -0.8148 - dice_coef: 0.8273 - acc: 0.8355 - mean_squared_error: 0.0082 - Jaccard_inde - ETA: 3:41 - loss: -0.8155 - dice_coef: 0.8280 - acc: 0.8342 - mean_squared_error: 0.0082 - Jaccard_index: 0 - ETA: 3:32 - loss: -0.8162 - dice_coef: 0.8287 - acc: 0.8365 - mean_squared_error: 0.0082 - Jaccard_index: 0.7085 - Specificity: 0 - ETA: 3:26 - loss: -0.8161 - dice_coef: 0.8286 - acc - ETA: 1:00 - loss: -0.8199 - dice_c\n",
      "Epoch 21/50\n",
      "1632/1632 [==============================] - 398s 244ms/step - loss: -0.8281 - dice_coef: 0.8404 - acc: 0.8366 - mean_squared_error: 0.0078 - Jaccard_index: 0.7256 - Specificity: 0.9744 - Sensitivity: 1.0055 - precision: 0.9229 - recall: 0.9184 - f1score: 0.9202 - val_loss: -0.7128 - val_dice_coef: 0.7250 - val_acc: 0.8260 - val_mean_squared_error: 0.0205 - val_Jaccard_index: 0.5718 - val_Specificity: 0.9626 - val_Sensitivity: 0.9987 - val_precision: 0.8954 - val_recall: 0.7045 - val_f1score: 0.7852 loss: -0.8252 - dice_coef: 0.8375 - acc: 0.8368 - mean_squared_error: 0.0079 - Jaccard_index: 0.7214 - Specificity: 0.9744 - Sens - ETA: 3:22 - loss: -0.8252 - d - ETA: 2:30 - loss: -0.8255 - dice_coef: 0.8378 - acc: 0.8348 - mean_squa - ETA: 1:40 - loss: -0.8266 - dice_coef: 0.8389 - acc: 0.8339 - mean_squared_error: 0.0079 - Jaccard_index: 0.7235 - Specific - ETA: 1:32 - loss: -0.8265 - dice_coef: 0.8388 - acc - E - ETA: 57s - loss: -0.8274 - dice_coef:  - ETA: 26s - loss: -0.8279 - dice_coef: 0.8402 - acc: 0.8364 - mean_squared_error: 0.0078 - Jaccard_index: 0.7254 - Specificity: 0.9744 - Sensitivity: 1.0055 - precision: 0.9226 - recall: 0.9184 - f1s - ETA: 24s - loss: -0.8280 - dice_coef: 0.8403 - acc: 0.8363 - mean_squared_error: 0.0078 - Jaccard_index: 0.7255 - Specificity: 0.9744 - Sensitivity: 1.0055 - precision: 0.9227 - re - ETA: 18s - loss: -0.8280 - dice_coef: 0.8402 - acc: 0.8364 - mean_squared_error: 0.0078 - Jaccard_index: 0.7254 - Specificity: 0.9744 - Sensitivity: 1.0055 - precision: 0.9227 - recall: 0.9184 - f1score: 0.92 - ETA: 18s - loss: -0.8280 - dice_coef: 0.8402 - acc: 0.83\n",
      "Epoch 22/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.8352 - dice_coef: 0.8473 - acc: 0.8369 - mean_squared_error: 0.0075 - Jaccard_index: 0.7360 - Specificity: 0.9744 - Sensitivity: 1.0048 - precision: 0.9280 - recall: 0.9201 - f1score: 0.9236 - val_loss: -0.7233 - val_dice_coef: 0.7353 - val_acc: 0.8252 - val_mean_squared_error: 0.0206 - val_Jaccard_index: 0.5845 - val_Specificity: 0.9636 - val_Sensitivity: 1.0015 - val_precision: 0.8686 - val_recall: 0.7382 - val_f1score: 0.7951 acc: 0.8423 - mean_squared_error: 0.0076 - Jaccard_index: 0.7408 - Specificity: 0.9737 - Sensitivity: 1.0051 - prec - ETA: 4:07 - loss: -0.8360 - dice_coef: 0.8482 - acc: 0.8423 - mean_squared_error: 0.0076 - Jaccard_index: 0.7372 - Specificity: 0.9741 - Sensitivity: 1.0051  - ETA: 4:06 - loss: -0.8360 - dice_coef: 0.8482 - acc: 0.8408 - ETA: 3:55 - loss: -0.8346 - dice_coef: 0.8467 - acc: 0.8364 - mean_squared_error: 0.0077 - Jaccard_index: 0.7351  - ETA: 3:48 - loss: -0.8334 - dice_coef: 0.8456 - acc: 0.8363 - mean_squared_error: 0.0077 - Jaccard_index: 0.7334 - Specificity: 0.9742 - Sensitivity: 1.0051 - precision: 0.9266 - recall: 0.9207 - f1score - ETA: 3:48 - loss: -0.8334 - - ETA: 3:37 - loss: -0.8333 - dice_coef: 0.8455 - acc: 0.8384 - mean_squared_error: 0.0077 - Jaccard_index: 0.7333 - Specificity: 0.9743 - Sensitivity: 1.0051 - precision: - ETA: 3:33 - loss: -0.8337 - dice_coef: 0.8459 - acc: 0.8397 - mean_squared_error: 0.0076 - Jaccard_index: 0.7339 - Specificity: 0.9743 - Sensitivity: 1.0051 - precision: - ETA: 3:30 - loss: -0.8336 - dice_coef: 0.8458 - acc: 0.8390 - mean_squared_error: 0.0077 - Jaccard_index: 0.7337 - Specificity: 0.9743 - Sensitivity: 1.0051 - precision: 0.9266 - reca - ETA: 3: - ETA: 2:15 - loss: -0.8346 - dice_coef: 0.8467 - acc: 0.8358 - mean_squared_error: 0.0076 - Jaccard_index: 0.7351 - Specificity: 0.9744 - Sensitivity: 1.0049 - precision: 0.9271 - recall: 0.9200 - f1score: 0.92 - ETA: 2:15 - loss: -0.8346 - dice_coef: 0.8467 - acc: 0.8358 - mean_squared_error: 0.0076 - Jaccard_index: 0 - ETA: 2:05 - loss: - - ETA: 1:29 - loss: -0.8350 - dice_coef:  - ETA: 1:14 - loss: -0.8354 - dice_coef: 0.8475 - acc: 0.8366 - mean_squared_error: 0.0075 - Jaccard_index: 0.7363 - Specificity: 0.9744 - Sensitivity: 1.0049 - precision: 0.9278 - recall: 0.9206 - f - ETA: 1:12 - loss: -0.8353 - dice_coef: 0.8474 - acc: 0.8365 - mean_squared_error: 0.0075 - Jacc - ETA: 1:02 - loss: -0.8354 - dice_coef: 0.8475 - acc: 0.8373 - mean_squared_error: 0.0075 - Jaccard_ind - ETA: 45s - loss: -0.8353 - dice_coef: 0.8474 - acc: 0.8372 - mean_squared_error: 0.0075 - Jaccard_index: 0.7361 - Specif - ETA: 28s - loss: -0.8354 - dice_coef: 0.8475 - acc: 0.8368 - mean_squared_error: 0.0075 - Jaccard_index: 0.7363 - Specificity: 0.9744 - Sensitivity: 1.0049 - precision: 0.9279 - recall: 0.9 - ETA: 25s - loss: -0.8353 - dice_coef: 0.8474 - acc: 0.8366 - mean_squared_error: 0.0075 - Jaccard_index: 0.7361 - Specificity: 0.9744 - Sensitivity: 1.0049 - precision: 0.9280 - recall: 0.9203 - f1score: 0.9 - ETA: 24s - loss: -0.83\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.8397 - dice_coef: 0.8516 - acc: 0.8370 - mean_squared_error: 0.0074 - Jaccard_index: 0.7424 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: 0.9313 - recall: 0.9207 - f1score: 0.9256 - val_loss: -0.7153 - val_dice_coef: 0.7272 - val_acc: 0.8260 - val_mean_squared_error: 0.0206 - val_Jaccard_index: 0.5748 - val_Specificity: 0.9625 - val_Sensitivity: 0.9979 - val_precision: 0.8992 - val_recall: 0.7001 - val_f1score: 0.7838: 0.8442 - acc: 0.8280 - mean_squared_error: 0.0076 - Jaccard_index: 0.7310 - Specificity: 0.9747 - Se - ETA: 4:11 - loss: -0.8356 - dice_coef: 0.8476 - acc: 0.8354 - mean_squared_error: 0.0075 - Jaccar - ETA: 4:15 - loss: -0.8393 - dice_coef: 0.8513 - acc: 0.8352 - mean_squared_error: 0.0074 - Jaccard_index: 0.7418 - Specificity: 0.9742 - Sens - ETA: 4:10 - loss: -0.8393 - dice_coef: 0.8513 - acc: 0.8354 - mean_squared_error: 0.0075 - Jaccard_index: 0.7419 - Specificity: 0.9741 - Sensitivity: 1.0045 - precision: 0.9286 - recall: 0.92 - ETA: 4:11 - loss: -0.8386 - dice_coef: 0.8506 - acc: 0.8335 - mean_squared_error: 0.0076 - Jaccard_index: 0.7409 - Specificity: 0.9740 - Sensitivity: 1.0045 - precision: 0.9285 - recall: 0.9207 - - ETA: 4:11 - loss: -0.8392 - dice_coef: 0.8512 - acc: 0.8353 - mean_squared_error: 0.0075 - Jaccard_index: 0.7418 - Specificity: 0.9740 - Sensitivity: 1.0045 - precision: 0.9289 -  - ETA: 4:10 - loss: -0.8387 - dice_coef: 0.8507 - acc: 0.8367 - mean_squared_error: 0.0075 - Jaccard_index: 0.7410 - Specificity: 0.9741 - Sensitivity: 1.0045 - precision: 0.9289 - recall: 0. - ETA: 4:09 - loss: -0.8383 - dice_coef: 0.8503 - acc: 0.8346 - mean_squared_error: 0.0075 - Jaccard_index: 0.7403 - Specificity: 0.9741 - Sensitivity: 1.0045 - precision: 0.9285 - recall: 0.9210 - f1s - E - ETA: 3:17 - loss: -0.8371 - dice_coef: 0.8491 - acc: 0.8322 - mean_squared_error: 0.0074 - Jaccard_index: 0.7386 - Specificity: 0.9744 - Sensitivity: 1.0045 - precision: 0.9297 - recall: 0.91 - ETA: 3:15 - loss: -0.8371 - dice_coef: 0.8491 - acc: 0.8317 - mean_sq - ETA: 2:25 - loss: -0.8378 - dice_coef: 0.8498 - a - ETA: 2:11 - loss: -0.8383 - dice_coef: 0.8502 - acc: 0.8362 - mean_squared_error: 0.0074 - Jaccard_index: 0.7403 - Specificity: 0.9744 - Sensitivity: 1.0045 - precision: 0.9306 -  - ETA: 2:08 - loss: -0.8384 - dice_coef: 0.8504 - acc: 0.8358 - mean_squared_error: - ETA: 1:55 - ETA: 1:37 - loss: -0.8385 - dice_coef: 0.8505 - acc: 0.8356 - mean_squared_error: 0.0074 - Jaccar - ETA: 1:26 - los - ETA: 1:08 - loss: -0.8391 - dice_coef: 0.8511 - acc: 0.8357 - mean_squared_error: 0.0074 - Jaccard_index: 0.7416 - Specificity: 0.9744 - Sens - ETA: 1:02 - loss: -0.8389 - dice_coef: 0.8508 - acc: 0.8356 - mean_squared_error: 0.0074 - Jaccard_index: 0.7412 - Specificity: 0.9744 -  - ETA: 51s - loss: -0.8390 - dice_coef: 0.8509 - acc: 0.8359 - mean_squared_error: 0.0074 - Jaccard_index: 0.7414 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: 0.9310 - recall: 0.920 - ETA: 48s - loss: -0.8391 - dice_coef: 0.8510 - acc: 0.8359 - mean_squared_error: 0.0074 - Jaccard_index: 0.7415 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: 0.9311 - recall: 0.9206 - f1s - ETA: 45s - loss: -0.8391 - dice_coef: 0.8511 - acc: 0.8359 - mean_squared_error: 0.0074 - Jac - ETA: 24s - loss: -0.8393 - dice_coef: 0.8513 - acc: 0.8360 - mean_squared_error: 0.0074 - Jaccard_index: 0.7419 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: - ETA: 16s - loss: -0.8396 - dice_coef: 0.8516 - acc: 0.8364 - mean_squared_error: 0.0074 - Jaccard_index: 0.7424 - ETA: 4s - loss: -0.8398 - dice_coef: 0.8518 - acc: 0.8368 - mean_squared_error: 0.0074 - Jaccard_index: 0.7427 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: 0.9 - ETA: 0s - loss: -0.8397 - dice_coef: 0.8516 - acc: 0.8370 - mean_squared_error: 0.0074 - Jaccard_index: 0.7425 - Specificity: 0.9744 - Sensitivity: 1.0044 - precision: 0.9313 - recall: 0.9207 - f1score\n",
      "Epoch 24/50\n",
      "1632/1632 [==============================] - 403s 247ms/step - loss: -0.8442 - dice_coef: 0.8560 - acc: 0.8372 - mean_squared_error: 0.0071 - Jaccard_index: 0.7492 - Specificity: 0.9745 - Sensitivity: 1.0041 - precision: 0.9348 - recall: 0.9230 - f1score: 0.9285 - val_loss: -0.7074 - val_dice_coef: 0.7191 - val_acc: 0.8264 - val_mean_squared_error: 0.0209 - val_Jaccard_index: 0.5657 - val_Specificity: 0.9617 - val_Sensitivity: 0.9956 - val_precision: 0.9181 - val_recall: 0.6732 - val_f1score: 0.7727red_error: 0.0072 - Jaccar - ETA: 1:33 - loss: -0.8433 - dice_coef: 0.8551 - acc: 0.8364 - mean_squared_error: - ETA: 1:21 - loss: -0.8432 - dice_coef: 0.8550 - acc: 0.8361 - mean_squared_error: 0.0072 - Jaccard_index: 0.7477 - Specificity: 0.9745 - Sensitivity: 1.0041 - pr -  - ETA: 54s - loss: -0.8432 - dice_coef: 0.8550 - acc: 0.8367 - mean_squared_error: 0.0072 - Jaccard_index: 0.7476 - Specificity: 0.9745 - Sensitivity: 1.0041 - precision: 0.9342 - recall: 0.9226 - f1score: 0 - ETA: 53s - loss: -0.8433 - dice_coef: 0.8551 - acc: 0.8369 - mean_squared_error: 0.0072 - Jaccard_index: 0.7477 - Specificity: 0.9745 - Sensitivity: 1.0041 - precision: 0.9342 - recall: 0.9 - ETA: 49s - loss: -0.8434 - dice_coef: 0.8552 - acc: 0.8369 - mean_squared_error: 0.0072 - Jaccard_index: 0.7478 - Specificity: 0.9745 - Sensitivity: 1.0041 - precision: 0.9344 - recall: - ETA: 44s - loss: -0.8434 - dice_coef: 0.8552 - acc: 0.8370 - mean_squared_error: 0.0072 - Jaccard_index: 0.7479 - Specificity: 0.9745 - Sensit - ETA: 32s - loss: -0.8436 - dice_coef: 0.8554 - acc: 0.8368 - mean_squared_error: 0.0071 - Jaccard_index: 0.7482 - Specificity: 0.9745 - Sensitivity: 1.0041 - precision: 0.9344 - rec - ETA: 26s - \n",
      "Epoch 25/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.8483 - dice_coef: 0.8600 - acc: 0.8374 - mean_squared_error: 0.0069 - Jaccard_index: 0.7552 - Specificity: 0.9745 - Sensitivity: 1.0038 - precision: 0.9383 - recall: 0.9249 - f1score: 0.9313 - val_loss: -0.7219 - val_dice_coef: 0.7335 - val_acc: 0.8267 - val_mean_squared_error: 0.0200 - val_Jaccard_index: 0.5831 - val_Specificity: 0.9624 - val_Sensitivity: 0.9967 - val_precision: 0.9145 - val_recall: 0.6976 - val_f1score: 0.7879Specificity: 0.9745 - Sensitivity: 1.0039 - precision: 0.9382 - re - ETA: 2:53 - loss: -0.8488 - dice_coef: 0.8605 - acc: 0.8437 - mean_squared_error: 0.0069 - Jaccard_index: 0.7560 - Specificity: 0.9745 - Sensitivity: 1.0039 - precision: 0.9380 - recall: 0.9252 - f1score:  - ETA: 2:53 - loss: -0.8488 - dice_coef: 0.8605 - acc: 0.8437 - mean_squared_error: 0.0069 - Jaccard_index: 0.7560 - Spec - ETA: 2:45 - los - ETA: 1:50 - loss: -0.8479 - dice_coef: 0.8596 - acc: 0.8368 - m - ETA: 1: - ET - ETA: 59s - loss: -0.8482 - dice_coef: 0.8598 - acc: 0.8373 - mean_squared_error: 0.0069 - Jaccard_index: 0.7551 - Specificity: 0.9744 - Sensitivity: 1.0039 - preci - ETA: 12s - loss: -0.8484 - dice_coef: 0.8600 - acc: 0.8382 - mean_squared_error: 0.0\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.8506 - dice_coef: 0.8620 - acc: 0.8374 - mean_squared_error: 0.0068 - Jaccard_index: 0.7584 - Specificity: 0.9746 - Sensitivity: 1.0037 - precision: 0.9398 - recall: 0.9262 - f1score: 0.9327 - val_loss: -0.7217 - val_dice_coef: 0.7331 - val_acc: 0.8257 - val_mean_squared_error: 0.0206 - val_Jaccard_index: 0.5824 - val_Specificity: 0.9629 - val_Sensitivity: 0.9986 - val_precision: 0.8885 - val_recall: 0.7133 - val_f1score: 0.7875 dice_coef: 0.8590 - a - ETA: 3:51 - loss: -0.8490 - dice_coef: 0.8605 - acc: 0.8413 - mean_squared_error: 0.0068 - Jaccard_index: 0.7560 - Specificity: 0.9749 - Sensitivity: 1.0038 - precision: 0 - ETA: 3:48 - loss: -0.8489 - ETA: 2:55 - loss: -0.8498 - dice_coef: 0.8613 - acc: 0.8387 - mean_squared_error: 0.0068 - Jaccard_inde - ETA: 2:46 - loss: -0.8498 - dic - ETA: 2:30 - l - ETA: 1:1 - ETA: 52s - loss: -0.8504 - dice_coef: 0.8618 - acc: 0.8378 - mean_squared_error: 0.0068 - Jaccard_index: 0.7581 - Specificity: 0.9746  - ETA: 39s - loss: -0.8501 - dice_coef: 0.8616 - acc: 0.8370 - mean_squared_error: 0.0068 - Jaccard_index: 0.7577 - Specificity: 0.9746 - Sensitivity: 1.0037 - precisi - ETA: 30s - loss: -0.8502 - dice_coef: 0.8616 - acc: 0.8368 - mean_squared_error: 0.0068 - Jaccard_index: 0.7578 - Specificity: 0.9746 - Sensitivity: 1. - ETA: 20s - loss: -0.8503 - dice_coef: 0.8617 \n",
      "Epoch 27/50\n",
      "1632/1632 [==============================] - 399s 245ms/step - loss: -0.8535 - dice_coef: 0.8648 - acc: 0.8376 - mean_squared_error: 0.0066 - Jaccard_index: 0.7627 - Specificity: 0.9746 - Sensitivity: 1.0035 - precision: 0.9425 - recall: 0.9278 - f1score: 0.9348 - val_loss: -0.7135 - val_dice_coef: 0.7247 - val_acc: 0.8205 - val_mean_squared_error: 0.0248 - val_Jaccard_index: 0.5714 - val_Specificity: 0.9649 - val_Sensitivity: 1.0089 - val_precision: 0.7841 - val_recall: 0.7876 - val_f1score: 0.7828d_error: 0.0067 - Jaccard_in - ETA: 3:44 - loss: -0.8513 - dice_coef: 0.8627 - acc: 0.8429 - mean_squared_error: 0.0067 - Jaccard_index: 0 - ETA: 3:38 - loss: -0.8521 - dice_coef: 0.8635 - acc - ETA: 2:13 - loss: -0.8534 - dice_coef: 0.8647 - ETA: 1:58 - loss: -0.8537 - dice_coef: 0.8650 - acc: 0.8388 - mean_squared_error: 0.0067  - ETA: 1:47 - loss: -0.8536 - dice_coef: 0.86 - ETA: 1:32 - loss: -0.8533 - dice_coef: 0.8646 - acc: 0.8387 - mean_squared_error: 0.0066 - Jaccard_index: 0.7624 - Specificity: 0 - ETA: 1:24 - loss: -0.8533 - dice_coef: 0.8646 - acc: 0.8386 - mean_squared_error: 0.0066 - Jaccard_index: 0 - ETA:  - ETA: 53s - loss: -0.8534 - dice_coef: 0.8647 - acc: 0.8384 - mean_squared_error: 0.0066 - Jaccard_index: 0.7625 - Specificity: 0.9745 - Sensitivity: 1.0035 - precision: 0.9 - ETA: 46s  - ETA: 10s - loss: -0.8535 - dice_coef: 0.8648 - acc: 0.8377 - mean_squared_error: 0.0066 - Jaccard_index: 0.7626 - Specificity: 0.9746 - Sensitivity: 1.0035 - precision: 0.9425 - recall: 0.92 - ETA: 8s - loss: -0.8536 - dice_coef: 0.8649 - acc: 0.8376 - mean_squared_error: 0.0066 - Jaccard_index: 0.7628 - Spec\n",
      "Epoch 28/50\n",
      "1632/1632 [==============================] - 402s 246ms/step - loss: -0.8553 - dice_coef: 0.8664 - acc: 0.8376 - mean_squared_error: 0.0065 - Jaccard_index: 0.7652 - Specificity: 0.9746 - Sensitivity: 1.0034 - precision: 0.9440 - recall: 0.9298 - f1score: 0.9366 - val_loss: -0.7122 - val_dice_coef: 0.7233 - val_acc: 0.8267 - val_mean_squared_error: 0.0206 - val_Jaccard_index: 0.5708 - val_Specificity: 0.9619 - val_Sensitivity: 0.9954 - val_precision: 0.9200 - val_recall: 0.6742 - val_f1score: 0.7744.9747 - Sens - ETA: 2:47 - loss: -0.8536 - dice_coef: 0.8648 - acc: 0.8347 - mean_squared_error: 0.0066 - Jaccard_index: 0.7626 - Specificity: 0.9747 - Sensitivity: 1.0034 - precision: - ETA: 2:43 - loss: -0.8534 - dice_coef: 0.8645 - acc: 0.8347 - mean_squared_error: 0.0066 - Jaccard_index: 0.7623 - Specificity: 0.9747 - Sensitivity: 1.0034 - precision: 0.9431 - recall - ETA: 2:40 - loss: -0.8534 - dice_coef: 0.8646 - acc: 0.8351 - ETA: 2:08 - loss: -0.8540 - dice_coef: 0.8652 - acc: 0.8378 - mean_squared_error: 0.0065 - Jaccard_index: 0.7633 - Specificity: 0.9747 -  - ETA: 2:01 - loss: -0.8542 - dice_coef: 0.8654 - acc: 0.8374 - mean_squared_error: 0.006 - ETA: 1:50 - loss: -0.8544 - dice_coef: 0.8656 - acc: 0.8375 - mean_squared_error: 0.0065 - Jaccard_index: 0.7639 - Specificity: 0.9747 - Sensitivity: 1.0034 - precision: 0.9436 - recall:  - ETA: 1:48 - loss: -0.8544 - dice_coef: 0.8656 - acc: 0.8371 - mean_squared_error: 0.0065 - Jaccard_index: 0.7639 - Specificity: 0.9747 - Sensitivity: 1.0034 - prec - ETA: 1:24 - loss: -0.8546 - dice_coef: 0.8658 - acc: 0.8374 - mean_squared_error: 0.0065 - Jaccard_index: 0.7642 - Specificity: 0.9747 - Sensitivity: 1.0034 - precision: 0.9437 - recall: 0.9296 - f - ETA: 1:23 - loss: -0.8546 - dice_coef: 0.8657 - acc: 0.8376 - mean_squared_error: 0.0065 - Jaccard_index: 0.7641 - Specificity: 0.9748 - Sensitivity: 1.0034 - precision: 0.9437 - recall: 0.9295 - f1score: 0. - ETA - ETA: 1:04 - loss: -0.8546 - dice_coef: 0.8658 - acc: 0.8373 - mean_squared_error: 0.0065 - Jaccard_index: 0.7642  - ETA: 51s - loss: -0.8549 - dice_coef: 0.8661 - acc: 0.8372 - mean_squared_error: 0 - ETA: 28s - loss: -0.8551 - dice_coef: 0.8663 - acc: 0.8380 - mean_squared_error: 0.0065 - Jaccard_index: 0.7649 - Specificity: 0.9747 - Sensitivity: 1.0034 - precision: 0.9439 - recall: 0.9297 - f1sc - ETA: 26s - loss: -0.8551 - dice_coef: 0.8663 - acc: 0.8380 - mean_squared_err - ETA: 5s - loss: -0.8553 - dice_coef: 0.8664 - acc: 0.8377 - mean_squared_error: 0.0065 - Jaccard_index: 0.7652 - Specificity: 0.9746 - Sensitivity: 1.0034 - precision: 0.9440 -  - ETA: 2s - loss: -0.8553 - dice_coef: 0.8664 - acc: 0.8379 - mean_squared_error: 0.0065 - Jaccard_index: 0.7652 - Specificity: 0.9746 - Sensitivity: 1.0034 - precision: 0.9440 - \n",
      "Epoch 29/50\n",
      "1632/1632 [==============================] - 397s 244ms/step - loss: -0.8574 - dice_coef: 0.8683 - acc: 0.8377 - mean_squared_error: 0.0064 - Jaccard_index: 0.7681 - Specificity: 0.9746 - Sensitivity: 1.0033 - precision: 0.9457 - recall: 0.9308 - f1score: 0.9379 - val_loss: -0.7259 - val_dice_coef: 0.7368 - val_acc: 0.8262 - val_mean_squared_error: 0.0203 - val_Jaccard_index: 0.5872 - val_Specificity: 0.9628 - val_Sensitivity: 0.9978 - val_precision: 0.9001 - val_recall: 0.7117 - val_f1score: 0.79122 - Specificity: 0.9745 - Sensitivity: 1.0033 - precision: 0.9459 - recall: 0.9311 - f1sco - ETA: 1:41 - loss: -0.8574 - dice_coef: 0.8684 - acc: 0.8351 - mean_squared_error: 0.0064 - Jaccar - ETA: 1:31 - loss: -0.8574 - dice_coef: 0.8684 - acc: 0.8353 - mean_squared_error: 0.0064 - Jaccard_index: 0.7683 - Specificity: 0.9745 - Sensitivity: 1.0033  - ETA: 1:26 - loss: -0.8575 - dice_coef: 0.8685 - acc: 0.8353 - mean_squared_error: 0.0064 - Jaccard_index: 0.7683 - Sp - ETA: 1:17 - loss: -0 - ETA: 23s - loss: -0.8572 - dice_coef: 0.8681 - acc: 0.8370 - mean_squared_error: 0.0064 - Jaccard_index: 0.7678 - Specificity: 0.9746 - Sensitivi - ETA: 11s - loss: -0.8572 - dice_coef: 0.8682 - acc: 0.8375 - mean_squared_error: 0.0064 -\n",
      "Epoch 30/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.1580 - dice_coef: 0.1688 - acc: 0.7136 - mean_squared_error: 0.1611 - Jaccard_index: 0.0941 - Specificity: 0.9613 - Sensitivity: 1.1197 - precision: 0.1407 - recall: 0.3429 - f1score: 0.1834 - val_loss: -0.2375 - val_dice_coef: 0.2482 - val_acc: 0.6726 - val_mean_squared_error: 0.1958 - val_Jaccard_index: 0.1431 - val_Specificity: 0.9602 - val_Sensitivity: 1.1668 - val_precision: 0.1773 - val_recall: 0.6063 - val_f1score: 0.271943 - mean_squared_error: 0.1606 - Jaccard_index: 0.0942 - Specificity: 0.9613 - Sensitivity: 1.1192 - precision: 0.1411 - recall: 0.3421 - ETA: 2s - loss: -0.1580 - dice_coef: 0.1688 - acc: 0.7140 - mean_squared_error: 0.1608 - Jaccard_index: 0.0941 - Specificity: 0.9613 - Sensitivity: 1.1194 - precision: 0.1409 - recall\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 402s 246ms/step - loss: -0.8383 - dice_coef: 0.8489 - acc: 0.8348 - mean_squared_error: 0.0101 - Jaccard_index: 0.7460 - Specificity: 0.9743 - Sensitivity: 1.0061 - precision: 0.9230 - recall: 0.9169 - f1score: 0.9170 - val_loss: -0.7367 - val_dice_coef: 0.7472 - val_acc: 0.8263 - val_mean_squared_error: 0.0196 - val_Jaccard_index: 0.6001 - val_Specificity: 0.9634 - val_Sensitivity: 0.9989 - val_precision: 0.8961 - val_recall: 0.7319 - val_f1score: 0.8028ard_index: 0.5864 - Specificity: 0.9725 - Sensitivity: 1.0261 - precision: 0.7569 -  - ETA: 4:07 - loss: -0.7095 - dice_coef: 0.7202 - acc: 0.8253 - mean_squared_error: 0.0341 - Jaccard_index: 0.5984 - Specificity: 0.9727 - Sensitivity: 1.0244 - prec - ETA: 3:49 - loss: -0.7641 - dic - ETA: 2:57 - loss: -0.8068 - dice_coef: 0.8175 - acc: 0.8286 - mean_squared_error: 0.0157 - Jaccar - ETA: 2:48 - loss: -0.8110 - dice_coef: 0.8216 - acc: 0.8310 - mean_squared_error: 0.0150 - Jaccard_index: 0.7140 - Specificity: 0.9740  - ETA: 2:41 - loss: -0.8136 - dice_coef: 0.8243 - acc: 0.8320 - mean_squared_error: 0.0145 - Jaccard_index: 0.7172 - Specificity: 0.974 - ETA: 2:34 - loss: -0.8159 - dice_coef: 0.8266 - acc: 0.8333 - mean_squared_error: 0.0141 - Jaccard_index: 0.7199 - Specificity: 0.9741 - Sensitivity: 1.0091 - prec - ETA: 2:30 - loss: -0.8171 - dice_coef: 0.8277 - acc: 0.8338 - mean_squared_error: 0.0139 - Jaccard_index: 0.7212 - Specificity: 0.9741 -  - ETA: 2:23 - loss: -0.8187 - dice_coef: 0.8294 - acc: 0.8336 - mean_squared_error: 0.0136 - Jaccard_index: 0.7230 - Specificity: 0.9742 - Sensitivity: 1.0087 - prec - ETA: 2:19 - loss: -0.8197 - dice_coef: 0.8304 - acc: 0.8335 - mean_squared_error: 0.0134 - Jaccard_index: 0.7242 - Specificity: 0.9742 - Sensitivity: 1.0086 - precision: 0.9014 - reca - ETA: 2:16 - loss: -0.8202 - dice_coef: 0.8309 - ETA: 2:01 - loss: -0.8235 - dice_coef: 0.83 - ETA: 1:46 - loss: -0.8265 - dice_coef: 0.8372 - acc: 0.8333 - mean_squared_error: 0.0121 - Jaccard_index: 0.7322 - Specificity: 0.9743 - Sensitivity: 1.0077 - precision: - ETA: 1:42 - loss: -0.8273 - dice_coef: 0.8379 - acc: 0.8333 - mean_squared_error: 0.0120 - Jaccard_index: 0.7331 -  - ETA: 1:33 - loss: -0.8288 - dice_coef: 0.8395 - acc: 0.8334 - mean_squared_error: 0.0118 - Jaccard_index: 0.7350 - Specificity: 0.9743 - Sensitivity: 1.0074 - precision: 0.9119 - recall: 0.9099 - f1score:  - ETA: 1:33 - loss: -0.8289 - dice_coef: 0.8395 - acc: 0.8334 - mean_squared_error: 0.0117 - Jaccard_index: - ETA: 1:04 - loss: -0.8328 - dice_coef: 0.8434 - acc: 0.8346 - mean_squared_error: 0.0111 - Jaccard_index: 0.7396 - Specificity: 0.9743 - Sensitivity: 1.0069 - precision: 0.9163 - recall: 0.9128 - f1score: 0. - ETA: 1:03 - loss: -0.8328 - dice_coef: 0.8435 - acc: 0.8346 - mean_squared_error: 0.0111 - Jaccard_index: 0.7397 - Specificity: 0.9743 - Sensitivity: 1.0069  - ETA: 57s - loss: -0.8335 - dice_coef: 0.8441 - acc: 0.8338 - mean_squared_error: 0.0109 - Jaccard_index: 0.7404 - Specificity: 0.9743 - Sensitivity: 1.0068 - precision: 0.9171 - r - ETA: 51s - loss: -0.8340 - dice_coef: 0.8447 - acc: 0.8342 - mean_squared_error: 0.0108 - Jaccard_index: 0.7410 - Specificity: 0.9743 - Sensitivity: 1.0067 - precisi - ETA: 43s - loss: -0.8347 - dice_coef: 0.8454 - acc: 0.8343 - mean_squared_error: 0.0107 - Jaccard_index: 0.7418 - Specificity: 0.9743 - Sensitivity: 1.0066 - precision: 0.9187 - ETA: 37s - loss: -0.8354 - dice_coef: 0.8460 - acc: 0.8347 - mean_squared_error: 0.0106 - Jaccard_index: 0.7426 - Specificity: 0.9743 - Sensitivity: 1.0065 - p - ETA: 27s - loss: -0.8361 - dice_coef: 0.8468 - acc: 0.8350 - mean_squared_error: 0.0104 - Jaccard_index: 0.7434 - Specificity: 0.9743 - Sensitivit - ETA: 16s - loss: -0.8370 - dice_coef: 0.8476 - acc: 0.8349 - mean_squared_error: 0.0103 - Jaccard_index: 0.7444 - Specificity: 0.9743 - Sensitivity: 1.0063 - precision: 0.9215 - recall: 0.9160 - f1score: 0.9 - ETA: 15s - loss: -0.8370 - dice_coef: 0.8477 - acc: 0.8348 - mean_squared_error: 0.0103 - Jaccard_index: 0.7444 - Specificity: 0.9743 - Sensitivity: 1.0063 - precision: 0.9215 - recall: 0.9160 - f - ETA: 13s - loss: -0.8373 - dice_coef: 0.8480 - acc: 0.8352 - mean_squared_error: 0.0102 - Jaccard_index: 0.7448 - Specificity: 0.9743 - Sensitivity: 1.0062 - precision: 0.9218 - recall: 0.9161 - f1s - ETA: 10s - loss: -0.8375 - dice_coef: 0.8481 - acc: 0.8353 - mean_squared_error: 0.0102 - Jaccard\n",
      "Epoch 32/50\n",
      "1632/1632 [==============================] - 401s 246ms/step - loss: -0.8611 - dice_coef: 0.8716 - acc: 0.8379 - mean_squared_error: 0.0061 - Jaccard_index: 0.7733 - Specificity: 0.9747 - Sensitivity: 1.0032 - precision: 0.9490 - recall: 0.9345 - f1score: 0.9415 - val_loss: -0.7100 - val_dice_coef: 0.7204 - val_acc: 0.8266 - val_mean_squared_error: 0.0210 - val_Jaccard_index: 0.5673 - val_Specificity: 0.9616 - val_Sensitivity: 0.9948 - val_precision: 0.9248 - val_recall: 0.6687 - val_f1score: 0.7720ccard_index: 0.7774 - Specificity: - ETA: 4:10 - loss: -0.8633 - dice_coef: 0.8738 - acc: 0.8321 - mean_squared_er - ETA: 4:06 - loss: -0.8627 - dice_coef: 0.8732 - acc: 0.8383 - mean_squared_error: 0.0061 - Jaccard_index: 0.7758 - Specificity: 0.9746 - Sensitivity: 1.0033 - precision: 0.9494 - recall: 0.9367 - ETA: 4:06 - loss: -0.8631 - dice_co - ETA: 3:38 - loss: -0.8611 - dice_coef: 0.8716 - acc: 0.8352 - mean_squared_error: 0.0061 - Jaccard_index: 0.7733 - Specif - - ETA: 2:33 - loss: -0.8600 - dice_coef: 0.8706 - acc: 0.8365 - mean_squared_error: 0.0062 - Jaccard_index: 0.7717 - Specificity: 0.9749 - Sensitivity: 1.0032 -  - ETA: 2:29 - loss: -0.8602 - dice_coef: 0.8707 - acc: 0.8361 - mean_squared_error: 0.0062 - Jaccard_index: 0.7719 - Specificity: 0.9748 - Sensitivity: 1.0032 - pr - ETA: 2:24 - loss: -0.8605 - dice_coef: 0.8710 - acc: 0.8365 - mean_squared_error: 0.0062 - Jaccard_index: 0.7724 - Specificit - E - ETA: 1:56 - loss: -0.8609 - dice_coef: 0.8715 - acc: 0.8376 - mean_squared_error: 0.0061 - Jaccard_index: 0.7731 - Specificity: 0.9747 - Sensitivity: 1.0032 - precision: 0.9 - ETA: 1:53 - loss: -0.8608 - dice_coef: 0.8713 - ETA: 57s - loss: -0.8613 - dice_coef: 0.8718 - acc: 0.8368 - mean_squared_error: 0.0061 - Jaccard_index: 0.7735 - Specificity: 0.9746 - Sensitivity: 1.0032 - precision: 0.9490 - recall: 0.9346 - f1score - ETA: 55s - loss: -0.8613 - dice_coef: 0.8718 - acc: 0.8369 - mean_squared_error: 0.0061 - Jaccard_index: 0.7735 - Specificity: 0.9746  - ETA: 41s - loss: -0.8611 - dice_coef: 0.8716 - acc: 0.8373 - mean_squared_error: 0.0061 - Jaccard_index: 0.7732 - ETA: 23s - loss: -0.8612 - dice_coef: 0.8717 - acc: 0.8373 - mean_squared_error: 0.0061 - Jaccard_index: 0.7735 - Specificity: 0.9746 - Sensitivity: 1.0032 - precision: 0.9490 - recall: 0.9346 - f1score: - ETA: 22s - loss: -0.8612 - dice_coef: 0.8717 - acc: 0.8371 - mean_squared_error: 0.0061 - Jaccard_index: 0.7734 - Specificity: 0.9746 - Sensitivity: 1.0032 - precision: 0.9490 - recall: 0.9345 - f1 - ETA: 19s - loss: -0.8612 - dice_coef: 0.8716 - acc: 0.8371 - mean_squared_ - ETA: 2s - loss: -0.8611 - dice_coef: 0.8716 - acc: 0.8377 - mean_squared_error: 0.0061 - Jaccard_index: 0.7733 - Specificity: 0.9747 - Sensitivity: 1.0032 - precision: 0.9490 - recall\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 397s 243ms/step - loss: -0.8635 - dice_coef: 0.8738 - acc: 0.8380 - mean_squared_error: 0.0060 - Jaccard_index: 0.7768 - Specificity: 0.9748 - Sensitivity: 1.0032 - precision: 0.9510 - recall: 0.9365 - f1score: 0.9435 - val_loss: -0.7223 - val_dice_coef: 0.7326 - val_acc: 0.8266 - val_mean_squared_error: 0.0203 - val_Jaccard_index: 0.5823 - val_Specificity: 0.9623 - val_Sensitivity: 0.9964 - val_precision: 0.9138 - val_recall: 0.6944 - val_f1score: 0.7853squared_error: 0.0060 - Ja - ETA: 2:54 - loss: -0.8641 - dice_coef: 0.8745 - acc: 0.8378 - mean_squared_error: 0.0060 - Jaccard_in - ETA: 1:47 - loss: -0.8636 - dice_coef: 0.8739 - acc: 0.8379 - mean_squared_error: 0.0060 - Jaccard_index: 0.7769 - Specificity: 0.9746 - Sensitivity: 1.0032 - precision: 0.9507 - reca - ETA: 1:44 - loss: -0.8638 - dice_ - ETA: 1:28 - loss: -0.8638 - dice_coef: 0.8742 - acc: 0.8381 - mean_squared_error: 0.0060 - Jaccard_index: 0.7774 - Specificity: 0.9746 - Sensitivity: 1.0032 - precision: 0.9508 - recall: 0.9365 - f1score - E - ETA: 38s - loss: -0.8635 - dice_coef: 0.8739 - acc: 0.8374 - mean_squared_error: 0.0060 - Jaccard_index: 0.7768 - Specificity: 0.9747 - Sensitivity: 1.0032 - precision: 0.9509 - recall: 0.9365 - f1score: 0.9 - ETA: 37s - loss: -0.8636 - dice_coef: 0.8739 - acc: 0.8375 - mean_squared_error: 0.0060 - Jaccard_index: 0.7769 - Specif - ETA: 21s - loss: -0.8635 - dice_coef: 0.8738 - acc: 0.8370 - mean_squared_error: 0.0060 - Jaccard_index: 0.7767 - Specificity: 0.9747 - Sensitivity: 1.0032 - precision: 0.9510 - recal - ETA: 16s - loss: -0.8635 - dice_coef: 0.8739 - acc: 0.8378 - mean_squ - ETA: 0s - loss: -0.8635 - dice_coef: 0.8739 - acc: 0.8380 - mean_squared_error: 0.0060 - Jaccard_index: 0.7768 - Specificity: 0.9748 - Sensitivity: 1.0032 - precision: 0.9510 - recall: 0.9365 - f1score: \n",
      "Epoch 34/50\n",
      "1632/1632 [==============================] - 401s 246ms/step - loss: -0.8644 - dice_coef: 0.8745 - acc: 0.8380 - mean_squared_error: 0.0059 - Jaccard_index: 0.7779 - Specificity: 0.9748 - Sensitivity: 1.0032 - precision: 0.9516 - recall: 0.9374 - f1score: 0.9442 - val_loss: -0.7192 - val_dice_coef: 0.7293 - val_acc: 0.8264 - val_mean_squared_error: 0.0207 - val_Jaccard_index: 0.5788 - val_Specificity: 0.9622 - val_Sensitivity: 0.9963 - val_precision: 0.9116 - val_recall: 0.6921 - val_f1score: 0.7823 dice_coef: 0.8720 - acc: 0.8298 - mean_squared_error: 0.0059 - Jaccard_index: 0.7739 - Specificity: 0.9749 - Sensitivity: 1.0032 - precision: - ETA: 4:09 - loss: -0.8618 - dice_coef: 0.8721 - acc: 0.8342 - mean_squared_error: 0.0059 - Jaccard_index: 0.7740 - Specificity: 0.9751 - Sensitivity: 1.0033 - precision: 0.9517 - recall: 0.9379 - f1sco - ETA: 4:09 - loss: -0.8623 - dice_coef: 0.8725 - acc: 0.8350 - mean_squared_error: 0.0059 - Jaccard_index: 0.7747 - Specificity: 0.9750 - Sensitivity: 1.0033 - precision: 0.9518 - recall: 0.9382 - - ETA: 4:08 - loss: -0.8623 - dice_coef: 0.8725 - acc: 0.8358 - - ETA: 3:59 - loss: -0.8628 - dice_coef: 0.8730 - acc: 0.8350 - mean_squared_error: 0.0060 - Jaccard_ - ETA: 3:52 - loss: -0.8624 - dice_coef: 0.8726 - acc: 0.8354 - mea - ETA: 3:43 - loss: -0.8628 - dice_coef: 0.8730 - acc: 0.8347 - mean_squared_error: 0.0060 - Jaccard_index: 0.7755 - Specificity: 0.9746 - Sensitivity: 1.0033 - precision: - ETA: 3:39 - loss: -0.8632 - dice_coef: 0.8735 - acc: 0.8350 - mean_squared_error: 0.0060 - Jaccard_index: 0.7 - ETA: 3:30 - loss: -0.8630 - dice_coef: 0.8732 - acc: 0.8349 - mean_squared_error: 0.0060 - Jaccard_index: 0.7758 - Specificity: 0.9746 - Sensitivity: 1.0033 - precision: 0.9518 - recall: 0.93 - ETA: 3:29 - loss: -0.8630 - dice_coef: 0.8732 - acc: 0.8345 - mean_squared_error: 0.0060 - Jaccard_index: 0.7758 - Specificity: 0.9746 - Sensitivity: 1.0033 - precis - ETA: 3:25 - loss: -0.8633 - dice_coef: 0.8735 - acc: 0.8346 - mean_squared_error: 0.0059 - Jaccard_index: 0.7762 - Specificity: 0.9747 - Sensitivity: 1.0033 - precision: 0 - ETA: 3:22 - loss: -0.8635 - dice_coef: 0.8737 - acc: 0.8333 - mean_sq - ETA: 3:10 - loss: -0.8644 - dice_coef: 0.8747 - acc: 0.8348 - ETA: 2:56 - loss: -0.8646 - dice_coef: 0.8748 - acc: 0.8352 - mean_sq - ETA:  - ETA: 1:08 - loss: -0.8643 - dice_coef:  - ETA: 44s - loss: -0.8645 - dice_coef: 0.8747 - acc: 0.8376 - mean_squared_error: 0.0059 - Jaccard_inde - ETA: 25s - loss: -0.8646 - dice_coef: 0.8748 - acc: 0.8379 - mean_squared_error: 0.0059 - Jaccard_index: 0.7783 - Specificity: 0.97 - ETA: 10s - loss: -0.8644 - dice_coef: 0.8746 - acc: 0.8378 - mean_squared_error: 0.0059 - Jaccard_index: 0.7780 - Specificity: 0.9748 - Sensitivity: 1.0032 - precision: 0.9516 -  - ETA: 7s - loss: -0.8645 - dice_coef: 0.8746 - acc: 0.8378 - mean_squared_error: 0.0059 - Jaccard_index: 0.7780 - Specificity: 0.9\n",
      "Epoch 35/50\n",
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.8662 - dice_coef: 0.8762 - acc: 0.8381 - mean_squared_error: 0.0058 - Jaccard_index: 0.7805 - Specificity: 0.9748 - Sensitivity: 1.0031 - precision: 0.9535 - recall: 0.9392 - f1score: 0.9461 - val_loss: -0.7296 - val_dice_coef: 0.7395 - val_acc: 0.8265 - val_mean_squared_error: 0.0199 - val_Jaccard_index: 0.5908 - val_Specificity: 0.9627 - val_Sensitivity: 0.9972 - val_precision: 0.9097 - val_recall: 0.7099 - val_f1score: 0.7938error: 0.0 - ETA: 4:12 - loss: -0.8651 - dice_coef: 0.8752 - acc: 0.8299 - mean_squared_error: 0.0058 - Jaccard_index: 0.7789 - Specificity: 0.9743 - Sensitivity: 1.0033 - precision: 0.9549 - recall: 0.9414 - f - ETA: 4:12 - loss: -0.8661 - dice_coef: 0.8761 - acc: 0.8331 - mean_squared_error: 0.0058 - Jaccard_index: 0.7 - ETA: 4:08 - loss: -0.8679 - dice_coef: 0.8780 - acc: 0.8368 - mean_squared_error: 0.0057 - Jaccard_ - ETA: 4:02 - loss: -0.8681 - dice_coef: 0.8782 - acc: 0.8424 - mean_squared_error: 0.0057 - Jaccard_index: 0.7836 - Specificity: 0.9746 - Sensitivity: 1.0032 - precis - ETA: 4:00 - loss: -0.8684 - dice_coef: 0.8785 - acc: 0.8425 - mean_square - ETA: 2:20 - loss: -0.8657 - dice_coef: 0.8758 - acc: 0.8414 - mean_squared_error: 0.0058 - Jaccard_index: 0 - ETA: 2:11 - loss: -0.8659 - dice_coef: 0.8759 - acc: 0.8413 - mean_squared_error: 0.0058 - Jaccard_index: 0.7800 - Specificity: 0.9748 - Sensitivity: 1.0032 - pr - ETA: 2:06 - loss: -0.8659 - dice_coef: 0.8760 - acc: 0.8413 - mean_squared_error: 0.0058 - Jaccard_index: 0.7801 - Specificity: 0.9748 - Sensitivity: 1.0032 - precision: 0.9528 - recall: 0.9387 - f1s - ETA: 2:05 - loss: -0.8659 - dice_coef: 0.8760 - acc: 0.8415 - mean_squared_error: 0.0058 - Jaccard_index: 0.7801 - Specificity: 0.9748 - Sensitivity: - ETA: 2:00 - loss: -0.8659 - dice_coef: 0.8759 - acc: 0.8414 - mean_sq - ETA: 1:08 - loss: -0.8661 - dice_coef: 0.876 - ETA: 47s - loss: -0.8663 - dice_coef: 0.8763 - acc: 0.8393 - mean_squared_error: 0.0058 - Jaccard_index: 0.7806 - Specificity: 0.9748 - Sensit - ETA: 35s - loss: -0.8664 - dice_coef: 0.8764 - acc: 0.8396 - mean_squared_error: 0.0058 - Jaccard_index: 0.7808 - Specificity: 0.9 - ETA: 20s - loss: -0.8663 - dice_coef: 0.8763 - acc: 0.8392 - mean_squared_error: 0.0058 - Jaccard_index: 0.7806 - Specificity: 0.9748 - Sensitivity: 1.0031 - precision: 0.9534 - recall: 0.9392 - f1score: 0 - ETA: 19s - loss: -0.8663 - dice_coef: 0.8763 -\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 396s 243ms/step - loss: -0.8676 - dice_coef: 0.8774 - acc: 0.8382 - mean_squared_error: 0.0057 - Jaccard_index: 0.7824 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9545 - recall: 0.9404 - f1score: 0.9472 - val_loss: -0.6969 - val_dice_coef: 0.7067 - val_acc: 0.8266 - val_mean_squared_error: 0.0216 - val_Jaccard_index: 0.5518 - val_Specificity: 0.9609 - val_Sensitivity: 0.9934 - val_precision: 0.9361 - val_recall: 0.6425 - val_f1score: 0.7569\n",
      "Epoch 37/50\n",
      "1632/1632 [==============================] - 395s 242ms/step - loss: -0.8688 - dice_coef: 0.8786 - acc: 0.8383 - mean_squared_error: 0.0055 - Jaccard_index: 0.7843 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9557 - recall: 0.9421 - f1score: 0.9487 - val_loss: -0.7217 - val_dice_coef: 0.7313 - val_acc: 0.8268 - val_mean_squared_error: 0.0202 - val_Jaccard_index: 0.5806 - val_Specificity: 0.9621 - val_Sensitivity: 0.9956 - val_precision: 0.9243 - val_recall: 0.6871 - val_f1score: 0.7842 - dice_coef: 0.8796 - acc: 0.8375 - mean_squared_error: 0 - ETA: 2: - ETA: 55s - loss: -0.8688 - dice_coef: 0.8785 - acc: 0.8390 - mean_squared_error: 0.0055 - Jaccard_index: 0.7842 - Specificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.9556 - recall: 0.94 - ETA: 51s - loss: -0.8688 - dice_coef: 0.8785 - acc: 0.8392 - mean_squared_error: 0.0055 - Jaccard_index: 0.7842 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.955 - ETA: 45s - loss: -0.8689 - dice_coef: 0.8787 - acc: 0.8392 - mean_squared_error: 0.0055 - Jaccard_index: 0.7844 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9556 - recall: 0.9420 - f1score:  - ETA: 44s - loss: -0.8690 - dice_coef: 0.8787 - acc: 0.8393 - mean_squared_error: 0.0055 - Jaccard_index: 0.7845 - Specificit\n",
      "Epoch 38/50\n",
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.8696 - dice_coef: 0.8792 - acc: 0.8383 - mean_squared_error: 0.0055 - Jaccard_index: 0.7852 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9566 - recall: 0.9427 - f1score: 0.9495 - val_loss: -0.7165 - val_dice_coef: 0.7260 - val_acc: 0.8266 - val_mean_squared_error: 0.0206 - val_Jaccard_index: 0.5747 - val_Specificity: 0.9618 - val_Sensitivity: 0.9952 - val_precision: 0.9254 - val_recall: 0.6789 - val_f1score: 0.7787 - acc: 0.8443 - mean_squared_error: 0.0055 - Jaccard_index: 0.7893 - Specificity: 0.9745 - Sensitivity: 1.0030 - precision: 0.9573 - recall: 0.9434 - - ETA: 4:07 - loss: -0.8721 - dice_coef: 0.8817 - acc: 0.8439 - mean_squared_error: 0.0055 - Jaccard_index: 0.7 - ETA: 4:02 - loss: -0.8724 - dice_coef: 0.8821 - acc: 0.8409 - mea - ETA: 3:52 - loss: -0.8720 - dice_coef: 0.8817 - acc: 0.8398 - mean_squared_error: 0.0054 - Jaccard_index: 0.7892 - Specificity: 0.9746 - Sensitivity: 1.0030 - precision: 0.9572 - re - ETA: 3:50 - loss: -0.8717 - dice_coef: 0.8813 - acc: 0.8401 - mean_squared_error: 0.0055 - Jaccard_inde - ETA: 3:40 - loss: -0.8717 - dice_coef: 0.8813 - acc: 0.8405 - mean_squared_error: 0.0054 - Jaccard_index: 0.7886 - Specificity: 0.9747 - Sensitivity: 1.0030 - precision: 0.9572 - recall: 0.9432 - f1sco - ETA: 3:39 - loss: -0.8719 - dice_coef: 0.8815 - acc: 0.8396 - mean_squared_ - ETA: 3:29 - loss: -0.8709 - dice_coef: 0.8805 - acc: 0.8407 - mean_squared_error: 0.0055 - Jaccard_index: 0.7874 - Specificity: 0.9746 - Sensitivity: 1.0030 - precision: 0.9571 - recall: 0.9433 - f1sco - ETA: 3:28 - loss: -0.8708 - dice_coef: 0.8804 - acc: 0.8402 - mean_squared_error: 0.0055 - Jaccard_index: 0.7872 - Specificity: 0.9747 - Sensitivity: - ETA: 3:05 - loss: -0.8699 - dice_coef: 0.8795 - acc: 0.8391 - mean_squared_error: 0.0055 -  - ETA: 2:17 - loss: -0.8699 - dice_coef: 0.8795 - acc: 0.8395 - mean_squared_error: 0.0055 - Jaccard_index: 0.7857 - Specificity: 0.9747 -  - ETA: 2:11 - loss: -0.8698 - dice_coef: 0.8794 - acc: 0.8398 - mean_squared_error: 0.0055 - Jaccard_index: 0.7856 - Specificity: 0.9748 - Sensitivity: 1.0031 - precision: 0.9564 - recall: 0.9421 - f1sco - ETA: 2:10 - loss: -0.8698 - dice_coef: 0.8793 - acc: 0.8399 - mean_squared_error: 0.0055 - Jaccard_index: 0.7855 - Specificity: 0.9748 - Sensitivity: 1.0031 - precision: 0.9564 - recall:  - ETA: 1:48 - loss: -0.8691 - dice_coef: 0.87 - ETA: 1:33 - loss: -0.8695 - dice_coef: 0.8791 - acc: 0.8414 - mean_squared_error: 0.0055 - Jaccard_index: 0.7850 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9564 - re - ETA: 1:31 - loss: -0.8695 - dice_coef: 0.8791 - acc: 0.8414 - mean_squared_error: 0.0055 - Jaccard_index: 0.7850 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9564 - recall: 0.9422 - f1score - ETA: 1:30 - loss: -0. - ETA: 1:13 - loss: -0.8693 - dice_coef: 0.8789 - acc: 0.8395 - mean_squared_error: 0.0055 - Jaccard_index: 0.7847 - Specificity: 0.9749 - Sensitivity: - ETA: 36s - loss: -0.8694 - dice_coef: 0.8790 - acc: 0.8393 - mean_squared_error: 0.0055 - Jaccard_index: 0.7849 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9565 - recall: 0.9426 - f - ETA: 34s - loss: -0.8694 - dice_coef: 0.8790 - acc: 0.8394 - mean_squared_error: 0.0055 - Jaccard_index: 0.7848 - Specificity: 0.9749 - Sensitivity: 1.0031 - preci - ETA: 25s - loss: -0.8695 - dice_coef: 0.8791 - acc: 0.8395 - mean_squared_error: 0.0055 - Jaccard_index: 0.7850 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0. - ETA: 18s - loss: -0.8695 - dice_coef: 0.8790 - acc: 0.839\n",
      "Epoch 39/50\n",
      "1632/1632 [==============================] - 400s 245ms/step - loss: -0.8708 - dice_coef: 0.8802 - acc: 0.8384 - mean_squared_error: 0.0054 - Jaccard_index: 0.7869 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9576 - recall: 0.9440 - f1score: 0.9506 - val_loss: -0.7160 - val_dice_coef: 0.7253 - val_acc: 0.8270 - val_mean_squared_error: 0.0204 - val_Jaccard_index: 0.5738 - val_Specificity: 0.9617 - val_Sensitivity: 0.9945 - val_precision: 0.9341 - val_recall: 0.6709 - val_f1score: 0.7767d_index: 0.7868 - Specificity: 0.974 - ETA: 2:15 - loss: -0.8709 - dice_coef: 0.8803 - acc: 0.8411 - mean_square - ETA: 2:03 - loss: -0.8709 - dice_coef: 0.8803 - acc: 0.8394 - mean_sq - ETA: 1:50 - loss: -0.8709 - dice_coef: 0.8803 - - ETA: 1:35 - loss: -0.8707 - dice_coef: 0.8801 - acc: 0.8394 - mean_squared_error: 0.0054 - Jaccard_index: 0.7867 - Specifi - ETA: 38s - loss: -0.8706 - dice_coef: 0.8800 - acc: 0.8392 - mean_squared_error: 0.0054 - Jaccard_index: 0.7865 - Specificity: 0.9750 - Sensitivity: 1. - ETA: 27s - loss: -0.8706 - dice_coef: 0.8801 - acc: 0.8393 - mean_squared_error: 0.0054 - Jaccard_index: 0.7866 - Specificity: 0.9750 - Sensiti - ETA: 14s - loss: -0.8708 - dice_coef: 0.8802 - acc: 0.8391 - mean_squared_\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 399s 245ms/step - loss: -0.8721 - dice_coef: 0.8813 - acc: 0.8384 - mean_squared_error: 0.0053 - Jaccard_index: 0.7886 - Specificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.9585 - recall: 0.9450 - f1score: 0.9516 - val_loss: -0.7253 - val_dice_coef: 0.7344 - val_acc: 0.8268 - val_mean_squared_error: 0.0200 - val_Jaccard_index: 0.5847 - val_Specificity: 0.9622 - val_Sensitivity: 0.9958 - val_precision: 0.9243 - val_recall: 0.6919 - val_f1score: 0.7874ecificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.9584 - recall: 0.9451 - ETA: 3:08 - loss: -0.8722 - dice_coef: 0.8815 - acc: 0.8434 - - ETA: 2:54 - loss: -0.8722 - dice_coef: 0.88 - ETA: 2:39 - loss: -0.8720 - dice_coef: 0.8813 - acc: 0.8421 - mean_squared_error: 0.0053 - Jaccard_index: 0.7885 - Specificity: 0.9752 - Sensitivity: 1.0031 - precisio - ETA: 2:36 - loss: -0.8721 - dice_coef: 0.8814 - acc: 0.8417 - mean_squared_error: 0.0053 - Jaccard_index: 0.7888 - Specificity: 0.9751 - Sensitivity: 1.0031 - precision: 0.9581  - ETA: 2:33 - loss: -0.8723 - dice_coef: 0.8816 - acc: 0.8413 - mean_squared_error: 0.0053 - Jaccard_index: 0.7890 - Specificity: 0.9751 - Sensitivity: 1.0030  - ETA: 2:28 - loss: -0.8724 - dice_coef: 0.8817 - acc: 0.8405 - mean_squared_error: 0.0053 - Jaccard_index: 0.7892 - Specificity: 0.9750 - Sensitivity: - ETA: 2:22 - loss: -0.8725 - dice_coef: 0.8818 - acc: 0.8408 - mean_squared_error: 0.005 - ETA: 2:11 - loss: -0.8728 - dice_coef: 0.8821 - acc: 0.8413 - mean_squared_error: 0.0053 - Jaccard_index: 0.7898 - Specificity: 0.9751  - ETA: 2:04 - loss: -0.8727 - dice_coef: 0.8819 - acc: 0.8407 - mean_squared_error: 0.0053 - Jaccard_index: 0.7896 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9583 - recall: 0.9450 - ETA:  - ETA: 1:44 - loss: -0.8727 - dice_coef: 0.8820 - acc: 0.8400 - mean_squared_error: 0.0053 - Jaccard_inde - ETA: 1:15 - loss: -0.8721 - dice_coef: 0.8814 - acc: 0.8395 - mean_squared_error: 0.0053 - Jaccard_index: 0.7887 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9582 - recall: 0.9446 - - ETA: 1:13 - loss: -0.8721 - dice_coef: 0.8814 - acc: 0.8394 - mean_squared_erro - ETA: 1:01 - loss: -0.8724 - dice_coef: 0.8816 - acc: 0.8391 - mean_squared_error: 0.0053 - Jaccard_index: 0.7892 - Specificity: 0.9750 - Sensitivity: 1.0030 - - ETA: 53s - loss: -0.8724 - dice_coef: 0.8817 - acc: 0.8390 - mean_squared_\n",
      "Epoch 41/50\n",
      "1632/1632 [==============================] - 401s 245ms/step - loss: -0.8732 - dice_coef: 0.8823 - acc: 0.8385 - mean_squared_error: 0.0052 - Jaccard_index: 0.7902 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9593 - recall: 0.9462 - f1score: 0.9526 - val_loss: -0.7201 - val_dice_coef: 0.7292 - val_acc: 0.8266 - val_mean_squared_error: 0.0205 - val_Jaccard_index: 0.5788 - val_Specificity: 0.9621 - val_Sensitivity: 0.9959 - val_precision: 0.9178 - val_recall: 0.6878 - val_f1score: 0.7818Sensitivity: 1.0030 - precision: 0 - ETA: 3:41 - loss: -0.8727 - dice_coef: 0.8819 - acc: 0.8358 - mean_squared_error: 0.0053 - Jaccard_index: 0.7895 - Specificity: 0.9749 - Se - ETA: 3:36 - loss: -0.8727 - dice_coef: 0.8819 - acc: 0.8341 - mean_squared_error: 0.0053 - Jaccard_index: - ETA: 3:26 - loss: -0.8727 - dice_coef: 0.8819 - acc: 0.8357 - mean_squared_error: 0.0053 - Jaccard_inde - ETA: 2:41 - loss: -0.8726 - dice_coef: 0.8817 - a - ETA: 2:26 - loss: -0.8727 - dice_coef - ETA: 2:11 - loss: -0.8727 - dice_coef: 0.8818 - acc: 0.8375 - mean_squared_error: 0.0053 - Jaccard_index: 0.7894 - Specificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.959 - ETA: 2:07 - loss: -0.8729 - dice_coef: 0.8820 - acc: 0.8377 - mean_squared_error: 0.0053 - Jaccard_index: 0.7898 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9589 - recall: 0.9460 - ETA: 2:06 - loss: -0.8729 - dice_coef: 0.8821 - acc: 0.8381 - mean_squared_error: 0.0053 - Jaccard_index: 0.7898 - Specificity: 0.9749 - Sensitivity: 1.0031 - precision: 0.9 - ETA: 2:02 - loss: -0.8729 - dice_coef: 0.8821 - acc: 0.8374 - mean_squared_error: 0.0053 - Jaccard_index: 0.7899 - Specificity: 0.9749 - Sensitivity: 1.0030 - precision: - ETA: 1:58 - loss: -0.8728 - dice_coef: 0.8819 - acc: 0. - ETA: 1:44 - loss: -0.8728 - dice_coef: 0.8819 - acc: 0.8387 - mean_squared_error: 0.0053 - Jaccard_index: 0.7896 - Specificity: 0.9749 - Sensitivity: 1 - ETA: 1:39 - loss: -0.8727 - dice_coef: 0.8818 - acc: 0.8388 - mean_squared_error: 0.0053 - Jaccard_index: 0.7895 - Specificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.9590 -  - ETA: 1:36 - loss: -0.8727 - dice_coef: 0.8818 - acc: 0.8387 - mean_squared_error: 0.0053 - Jaccard_index: 0.7894 - Specificity: 0.9750 - Sensitivity: 1.0031 - precision: 0.9590  - ETA: 1:33 - loss: -0.8727 - dice_coef: 0.8818 - acc: 0.8388 - mean_squared_error: 0.0052 - Jaccard_index: 0.7895 - Specificity: 0.9750 - Sensitivity: 1.0031 - prec - ETA: 1:28 - loss: -0.8728 - dice_coef: 0.8819 - acc: 0.8383 - mean_squared_error: 0.0052 - Jaccard_index: 0.7896 - Specificity: 0.9750 - Sensitivity: 1.0031 - prec - ETA: 1:24 - loss: -0.8727 - dice_coef: 0.8819 - acc: 0.8380 - mean_squared_error: 0.0052 - Jaccard_index: 0.7895 - Specificity: 0.9750 - Sensitivity: 1.0031 -  - ETA: 1:19 - loss: -0.8729 - d - ETA: 1:03 - loss: -0.8731 - dice_coef: 0.8822 - acc: 0.8380 - mean_squared_error: 0.0052 - Jaccard_index: 0.7900 - Specif - ETA: 49s - loss: -0.8732 - dice_coef: 0.8823 - acc: 0.8385 - mean_squared_error: 0.0052 - Ja - ETA: 28s - loss: -0.8731 - dice_coef: 0.8822 - acc: 0.8390 - mean_squared_error: 0.0052 - Jaccard_index: 0.7901 - Sp - ETA: 10s - loss: -0.8730 - dice_coef: 0.8821 - acc: 0.8381 - mean_squared_error: 0.0052 - Jaccar\n",
      "Epoch 42/50\n",
      "1632/1632 [==============================] - 397s 244ms/step - loss: -0.8742 - dice_coef: 0.8831 - acc: 0.8385 - mean_squared_error: 0.0051 - Jaccard_index: 0.7916 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9602 - recall: 0.9473 - f1score: 0.9536 - val_loss: -0.7258 - val_dice_coef: 0.7347 - val_acc: 0.8266 - val_mean_squared_error: 0.0201 - val_Jaccard_index: 0.5853 - val_Specificity: 0.9623 - val_Sensitivity: 0.9961 - val_precision: 0.9198 - val_recall: 0.6956 - val_f1score: 0.7879921 - Specificity: 0.9749 - Sensitivity: 1.0 - ETA: 3:46 - loss: -0.8748 - dice_coef: 0.8838 - acc: 0.8400 - mean_squared_error: 0.0051 - Jaccard_index: 0.7926 - Specificity: 0.9750 - Sensitivity: 1.0030 -  - ETA: 3:42 - loss: -0.8747 - dice_coef: 0.8837 - acc: 0.8380 - mean_sq - ETA: 3:32 - loss: -0. - ETA: 3:16 - loss: -0.8746 - dice_coef: 0.8836 - acc: 0.8397 - mean_squared_error: 0.0051 - Jaccard_index: 0 - ETA: 3:07 - - ETA - ETA: 2:13 - loss: -0.8733 - dice_coef: 0.8823 - acc: 0.8396 - mean_squa\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 402s 246ms/step - loss: -0.8753 - dice_coef: 0.8841 - acc: 0.8386 - mean_squared_error: 0.0051 - Jaccard_index: 0.7931 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9612 - recall: 0.9482 - f1score: 0.9545 - val_loss: -0.7287 - val_dice_coef: 0.7374 - val_acc: 0.8265 - val_mean_squared_error: 0.0200 - val_Jaccard_index: 0.5883 - val_Specificity: 0.9625 - val_Sensitivity: 0.9967 - val_precision: 0.9140 - val_recall: 0.7048 - val_f1score: 0.79198763 - dice_coef: 0.8852 - acc: 0.8364 - mean_squared_error: 0.0051 - Jaccard_index: 0.7947  - ETA: 3:10 - loss: -0.8758 - dic - ETA: 1:55 - loss: -0.8751 - dice_coef: 0.8840 - acc: 0.8357 - mean_squared_error: 0.0051 - Jaccard_index: 0.7929 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9609 - recall: 0.9483 - f1sco - ETA: 1:54 - loss: -0.8751 - dice_coef: 0.8839 - acc: 0.8357 - mean_squared_er - E - ETA: 1:23 - loss: -0.8752 - dice_coef: 0.8840 - acc: 0.8357 - mean_squared_error: 0.0051 - Jaccard_index: 0.7929 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9611 - recall: 0.9482 - f1score:  - ETA: 1:23 - loss: -0.8751 - dice_coef: 0.8839 - acc: 0.8357 - mean_squared_error: 0.0051 - Jaccard_index: 0.7928 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9610 - reca - ETA: 1:20 - loss: -0.8751 - dice_coef: 0.8839 - acc: 0.8353 - mean_squared_error: 0.0051 - Jaccard_index: 0.7927 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0 - ETA: 1:16 - loss: -0.8750 - dice_coef: 0.8838 - acc: 0.8353 - mean_squared_erro - ETA: 1:04 - loss: -0.8747 - dice_coef: 0.8835 - acc: 0.8359 - mean_s - ETA: 43s - loss: -0.8747 - dice_coef: 0.8835 - acc: 0.8370 - mean_squared_error: 0.0051 - Jaccard_index: 0.7921 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9610 - recall: 0.9480 - ETA: 40s - loss: -0.8748 - dice_coef: 0.8836 - acc: 0.8373 - mean_squared_error: 0.0051 - Jaccard_index: 0.7923 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9610 - recall: 0.94 - ETA: 36s - loss: -0.8749 - dice_coef: 0.8837 - acc: 0.8377 - mean_squared_error: 0.0051 - Jaccard_index: 0.7924 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0\n",
      "Epoch 44/50\n",
      "1632/1632 [==============================] - 398s 244ms/step - loss: -0.8758 - dice_coef: 0.8844 - acc: 0.8386 - mean_squared_error: 0.0050 - Jaccard_index: 0.7937 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9615 - recall: 0.9491 - f1score: 0.9552 - val_loss: -0.7318 - val_dice_coef: 0.7403 - val_acc: 0.8265 - val_mean_squared_error: 0.0199 - val_Jaccard_index: 0.5920 - val_Specificity: 0.9627 - val_Sensitivity: 0.9971 - val_precision: 0.9110 - val_recall: 0.7108 - val_f1score: 0.7949n_squared_error: 0.0050 - Jaccard_index: 0.7941 - Specif - ETA: 3:16 - loss: -0.8765 - dice_coef: 0.8853 - acc: 0.8420 - mean_squared_error: 0.0050  - ETA: 3:05 - loss: -0.8768 - dice_coef: 0.8855 - acc: 0.8428 - mean_squared_error: 0.0050 - Jaccard_index: 0.7953 - Specificity: 0.9 - ETA: 2:57 - loss: -0.8769 - dice_coef: 0.8856 - acc: 0.8419 - mean_squared_error: 0.0050 - Jaccard_index: 0.7955 - Specific - ETA: 2:50 - loss: -0.8770 - dice_coef: 0.8857 - acc: 0.8422 - mean_squared_error: 0.0050 - Jaccard_index: 0.7957 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9614 - recall: 0.94 - ETA: 2:29 - loss: -0.8765 - dice_coef: 0.8852 - acc: 0.8413 - mean_squared_error: 0.0050 - Jaccard_index: 0.7948 - Specificity: 0.9 - ETA: 34s - loss: -0.8758 - dice_coef: 0.8845 - acc: 0.8384 - mean_squared_error: 0.0050 - Jaccard_index: 0.7938 - Specificity: 0.9750 - Sensitivity: 1.0030 - precision: 0.9615 - recall: 0.9492 - ETA: 30s - loss: -0.8758 - dice_coef: 0.8845 - acc: 0.8387 - mean_squared_error: 0.0050 - Jaccard_index: 0.7938 - Specificity: 0.9750 - Sensitivity: 1.0030 - ETA: 21s - loss: -0.8757 - dice_coef: 0\n",
      "Epoch 45/50\n",
      "1632/1632 [==============================] - 397s 243ms/step - loss: -0.8769 - dice_coef: 0.8854 - acc: 0.8386 - mean_squared_error: 0.0049 - Jaccard_index: 0.7953 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9625 - recall: 0.9503 - f1score: 0.9562 - val_loss: -0.7218 - val_dice_coef: 0.7302 - val_acc: 0.8262 - val_mean_squared_error: 0.0207 - val_Jaccard_index: 0.5797 - val_Specificity: 0.9623 - val_Sensitivity: 0.9967 - val_precision: 0.9079 - val_recall: 0.6952 - val_f1score: 0.7829- Sens - ETA: 1:46 - loss: -0.8778 - dice_coef - ETA: 1:31 - loss: -0.8777 - dice_coef: 0.8862 - acc: 0.8380 - mean_squared_error: 0.0049 - Jaccard_index: 0.7965 - Specificity: 0.9750 -  - ETA: 1:24 - loss: -0.8776 - dice_coef: 0.8861 - acc: 0.8381 - mean_squared_error: 0.0049 - Jaccard_index: 0.7964 - Specificity: 0.9750 - Sensitivity: 1.0030 - pr - ETA: 1:20 - loss: -0.8777 - dice_coef: 0.8862 - acc: 0.8382 - mean_squared_error: 0.0049 - Jaccard_index: - ETA: 1:10 - loss: -0.8777 - dice_coef: 0.8862 - acc: 0.8383 - mean_squared_error: 0.0049 - Jaccard_index: 0.7965 - Specificity: 0.9750 - Sensitivity: 1.003 - ETA: 1:05 - loss: -0.8777 - dice_coef: 0.8862 - acc: 0.8382 - mean_squared_error: 0.0049 - Jaccard_index: 0.7965 - Specificity: 0.975 - ETA: 57s - loss: -0.8776 - dice_coef: 0.8862 - acc: 0.8382 - mean_squared_error: 0.0049 - Jaccard_index: 0.7964 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9626 - recall: 0.9504 - f1scor - ETA: 55s - loss: -0.8776 - dice_coef: 0.8861  - ETA: 26s - loss: -0.8771 - dice_coef: 0.8857 - acc: 0.8385 - mean_squared_ - ETA: 5s - loss: -0.8770 - dice_coef: 0.8855 - acc: 0.8384 - mean_squared_error: 0.0049 - Jaccard_index: 0.7953 - Specificity: 0.9751 - Sensitivit\n",
      "Epoch 46/50\n",
      "1632/1632 [==============================] - 398s 244ms/step - loss: -0.8776 - dice_coef: 0.8860 - acc: 0.8387 - mean_squared_error: 0.0049 - Jaccard_index: 0.7962 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9630 - recall: 0.9510 - f1score: 0.9569 - val_loss: -0.7295 - val_dice_coef: 0.7377 - val_acc: 0.8265 - val_mean_squared_error: 0.0201 - val_Jaccard_index: 0.5892 - val_Specificity: 0.9626 - val_Sensitivity: 0.9972 - val_precision: 0.9081 - val_recall: 0.7087 - val_f1score: 0.7918 acc: 0.8309 - mean_squared_error: 0.0050 - Jaccard_index: 0.7927  - ETA: 4:01 - loss: -0.8752 - dice_coef: 0.8836 - acc: 0.8421 - mean_squared_error: 0.0051 - Jaccard_index: 0.7923 - Specificity: 0.9752 - Sensitivity: 1.0033 - precision: 0.9 - ETA: 4:04 - loss: -0.8760 - dice_coef: 0.8845 - acc: 0.84 - ETA: 39s - loss: -0.8771 - dice_coef: 0.8854 - acc: 0.8379 - mean_squared_error: 0.0049 - Jaccard_index: 0.7953 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9629 - recall: 0.95 - ETA: 35s - loss: -0.8772 - dice_coef: 0.8856 - acc: 0.8381 - mean_s - ETA: 9s - loss: -0.8774 - dice_coef: 0.8858 - acc: 0.8381 - mean_squared_error: 0.0049 - Jaccard_index: 0.7959 - Specificity: 0.9751 - Sensitivity: 1.003 - ETA: 4s - loss: -0.8775 - dice_coef: 0.8859 - acc: 0.8387 - mean_squared_error: 0.0049 - Jaccard_index: 0.7960 - Specificity: 0.9751 - Sensitivity: 1.0030 - \n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 399s 245ms/step - loss: -0.8786 - dice_coef: 0.8868 - acc: 0.8387 - mean_squared_error: 0.0048 - Jaccard_index: 0.7975 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9638 - recall: 0.9519 - f1score: 0.9577 - val_loss: -0.7234 - val_dice_coef: 0.7316 - val_acc: 0.8267 - val_mean_squared_error: 0.0202 - val_Jaccard_index: 0.5818 - val_Specificity: 0.9622 - val_Sensitivity: 0.9958 - val_precision: 0.9218 - val_recall: 0.6908 - val_f1score: 0.78528 - Jaccard_index: 0.7964 - Sp - ETA: 3:21 - loss: -0.8775 - dice_coef: 0.8858 - acc: 0.8410 - mean_squared_error: 0.0048 - Jaccard_index: 0.7958 - Specificity: 0.9753 - Sensitivity: 1.0030 - precision: 0.9634 -  - ETA: 3:19 - loss: - ETA: 3:01 - loss: -0.8771 - dice_coef: 0.8854 - acc: 0.8378 - - ETA: 2:48 - loss: -0.8776 - dice_coef: 0.8858 - acc:  - ETA: 2:35 - loss: -0.8772 - dice_coef: 0.8854 - acc: 0.8361 - mean_squared_error: 0.0049 - Jaccard_in - ETA: 1:27 - loss: -0.8784 - dice_coef: 0.8866 - acc: 0.8369 - mean_squared_error: 0.0048 - Jaccard_index: 0.7972 - Specificity: 0.9751 - Sensitivity: 1 - ETA: 1:21 - loss: -0.8784 - dice_coef: 0.8867 - acc: 0.8364 - mean_squared_erro - ETA: 40s - loss: -0.8784 - dice_coef: 0.8867 - acc: 0.8369 - mean_squared_error: 0.0048 - Jaccard_inde - ETA: 20s - loss: -0.8786 - dice_coef: 0.8868 - acc: 0.8380 - mean_squared_error: 0.0048 - Ja - ETA: 4s - loss: -0.8786 - dice_coef: 0.8868 - acc: 0.8385 - mean_squared_error: 0.0048 - Jaccard_index: 0.7975 - Specificity: 0.9751 - Sensitivity: 1.0030 - pr\n",
      "Epoch 48/50\n",
      "1632/1632 [==============================] - 397s 243ms/step - loss: -0.8793 - dice_coef: 0.8874 - acc: 0.8387 - mean_squared_error: 0.0048 - Jaccard_index: 0.7984 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9643 - recall: 0.9527 - f1score: 0.9584 - val_loss: -0.7233 - val_dice_coef: 0.7314 - val_acc: 0.8259 - val_mean_squared_error: 0.0209 - val_Jaccard_index: 0.5809 - val_Specificity: 0.9625 - val_Sensitivity: 0.9975 - val_precision: 0.8979 - val_recall: 0.7049 - val_f1score: 0.7857ror: 0.0046 - Jaccard_index: 0.8010 - Specificity: 0.9756 - Sensitivity: 1.0032 - precision: - ETA: 4:00 - loss: -0.8818 - dice_coef: 0.8900 - acc: 0.8605 - mean_squa - ETA: 4:03 - loss: -0.8797 - dice_ - ETA: 3:33 - loss: -0.8811 - dice_coef: 0.8892 - acc: 0.8445 - mean_squared_error: 0.0047 - Jaccard_index: 0.8013 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9642 - recall: 0.9528 - ETA:  - ETA: 1:59 - loss: -0.8795 - dice_coef: 0.8877 - acc: 0.8397 - mean_squared_error: 0.0047 - Jaccard_index: 0.7988 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9643 - recall - ETA: 1:57 - loss: -0.8795 - dice_coef: 0.8876 - acc: 0.8390 - mean_squared_error: 0.0047 - Jaccard_index: 0.7987 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9643 - recall: 0.95 - ETA: 1:35 - loss: -0.8797 - dice_coef: 0.8878 - acc: 0.8394 - mean_squared_error: 0.0047 - Jaccard_index: 0.7991 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9643 - recall: 0.9528 - f1score - ETA: 1:35 - loss: -0.8798 - dice_coef: 0.8879 - acc: 0.8392 - mean_squared_error: 0.0047 - Jaccard_index: 0.7991 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.964 - ETA: 1:31 - loss: -0.8799 - dice_coef: 0.8880 - acc: 0.8396 - mean_squared_error: 0.0047 - Jaccard_index: 0.7994 - Specificity: 0.9751  - ETA: 1:25 - loss: -0.8798 - dice_coef: 0.8879 - acc: 0.8394 - mean_squared_error: 0.0048 - Jaccard_index: 0.7992 - Speci - ETA: 54s - loss: -0.8795 - dice_coef: 0.8876 - ETA: 24s - loss: -0.87\n",
      "Epoch 49/50\n",
      "1632/1632 [==============================] - 403s 247ms/step - loss: -0.8796 - dice_coef: 0.8875 - acc: 0.8388 - mean_squared_error: 0.0048 - Jaccard_index: 0.7986 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - recall: 0.9531 - f1score: 0.9586 - val_loss: -0.7316 - val_dice_coef: 0.7395 - val_acc: 0.8266 - val_mean_squared_error: 0.0201 - val_Jaccard_index: 0.5909 - val_Specificity: 0.9626 - val_Sensitivity: 0.9970 - val_precision: 0.9100 - val_recall: 0.7102 - val_f1score: 0.7935ean_squared_error: 0.0048 - Jaccard_index: 0.7991 - Specificity: 0.9753 - Sensitivit - ETA: 3:17 - loss: -0.8792 - dice_coef: 0.8872 - acc: 0.8433 - mean_squared_error: 0.0048 - Jaccard_index: 0.7981 - Specificity: 0.9753 - Sensitivity: 1.003 - ETA: 3:12 - loss: -0.8793 - dice_coef: 0.8873 - acc: 0.8425 - mean_squared_error: 0.0048 - Jaccard_index: 0.7982 - Sp - ETA: 3:04 - loss: -0.8800 - dice_coef: 0.8880 - acc: 0.8419 - mean_squared_error: 0.0048 - Jaccard_index: 0.7994 -  - ETA: 2:54 - loss: -0.8803 - dice_coef: 0.8883 - acc: 0.8427 - mean_squared_error: 0.0047 - Jaccard_in - ETA: 2:44 - loss: -0.8806 - dice_coef: 0. - ETA: 2:28 - loss: -0.8806 - dice_coef: 0.8886 - acc: 0.8421 - mean_squared_error: 0.0047 - Jaccard_index: 0.8004 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - recall:  - ETA:  - ETA: 1:09 - loss: -0.8798 - dice_coef: 0.8878 - acc: 0.8397 - mea - ETA: 51s - loss: -0.8799 - dice_coef: 0.8878 - acc: 0.8396 - mean_squared_error: 0.0047 - Jaccard_index: 0.7991 - Specificity: 0.9751 - Sensitivity: 1.0030 - precisi - ETA: 43s - loss: -0.8801 - dice_coef: 0.8880 - acc: 0.8397 - mean_squared_error: 0.0047 - Jaccard_index: 0.7994 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - recall: 0.9532 - f1scor - ETA: 41s - loss: -0.8801 - dice_coef: 0.8880 - acc: 0.8395 - mean_squared_error: 0.0047 - Jaccard_index: 0.7994 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - recall: 0 - ETA: 37s - loss: -0.8801 - dice_coef: 0.8880 - acc: 0.8396 - mean_squared_error: 0.0047 - Jaccard_index: 0.7995 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - - ETA: 31s - loss: -0.8799 - dice_coef: 0.8879 - acc: 0.8393 - mean_squared_error: 0.0047 - Jaccard_index: 0.7992 - Specificity: 0.9751 - Sensitivity: 1.0030 - precision: 0.9645 - recall: 0.9531 - f1 - ETA\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1632/1632 [==============================] - 404s 248ms/step - loss: -0.8804 - dice_coef: 0.8882 - acc: 0.8388 - mean_squared_error: 0.0047 - Jaccard_index: 0.7999 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9652 - recall: 0.9542 - f1score: 0.9596 - val_loss: -0.7355 - val_dice_coef: 0.7432 - val_acc: 0.8268 - val_mean_squared_error: 0.0196 - val_Jaccard_index: 0.5958 - val_Specificity: 0.9628 - val_Sensitivity: 0.9971 - val_precision: 0.9147 - val_recall: 0.7135 - val_f1score: 0.7980ificity: - ETA: 3:42 - loss: -0.8808 - dice_coef: 0.8886 - acc: 0.8386 - mean_squared_error: 0.0046 - Jaccard_index: 0.8005 - Specificity: 0.9755 - Sensitivit - ETA: 3:37 - loss: -0.8809 - dice_coef: 0.8887 - acc: 0.8398 - mean_squared_error: 0.0047 - Jaccard_index: 0.8006 - Specif - ETA: 3:30 - loss: -0.8807 - dice_coef: 0.8886 - acc: 0.8405 - mean_squared_error: 0.0046 - Jaccard_index: 0.8004 - Specificity: 0.9755 - Sensitivity: 1.0029 - precision: 0.9644 - reca - ETA: 3:27 - loss: -0.8806 - dice_coef: 0.8884 - acc: 0.8402 - mean_squared_error: 0.0047 - Jaccard_index: 0.8002 - Specificity: 0.9755 -  - ETA: 3:21 - loss: -0.8807 - dice_coef: 0.8885 - acc: 0.8413 - mean_squared_error: 0.0046 - Jaccard_index: 0.8004 - Specificity: 0.9755 - Sensitivity: 1.0029 - precision: 0.9645 -  - ETA: 3:18 - loss: -0.8810 - dice_coef: 0.8889 - acc: 0.8409 - mean_squared_error: 0.0046 - Jaccard_index: 0.8009 - Specificity: 0.9755 - Sensitivity: 1.0029 - precisio - ETA: 3:14 - loss: -0.8810 - dice_coef: 0.8888 - acc: 0.8405 - mean_squared_error: 0.0046 - Jaccard_index: 0.8008 - Specificity: - ETA: 3:06 - loss: -0.8810 - dice_coef: 0.8888 - ETA: 2:51 - loss: -0.8810 - dice_coef: 0.8889 - acc: 0.8407 - mean_squared_error: 0.0046 - Jaccard_index: 0.8009 - Specificity: 0.9754 - Sensitivity: - ETA: 2:26 - loss: -0.8812 - dice_coef: 0.8890 - acc: 0.8415 - mean_squared_error: 0.0046 - Jaccard_index: 0.8011 - Specificity: 0.9753 - Sensitivity: 1.0029 - precision: 0.9650 - recall: 0.9539 - f1score - ETA: 2:25 - loss: -0.8811 - dice_coef: 0.8890 - acc: 0.8416 - mean_squared_error: 0.0046 - Jaccard_index: 0.8010 - Specificity: 0.9753 - Sensitivity: 1.0029 - precision: 0.9650 - recall: 0.95 - ETA: 2:23 - loss: -0.8812 - dice_coef: 0.8890 - acc: 0.8417 - mea - ETA: 2:09 - loss: -0.8808 - dice_coef: 0.8887 - acc: 0.8410 - mean_squared_error: 0.0047 - Jaccard_index: 0.8005 - Specificity: 0.9753 - Sensitivity: 1.0030 - precision: 0.9650 - recall: 0. - ETA: 2:07 - loss: -0.8808 - dice_coef: 0.8887 - acc: 0.8411 - mean_squared_error: 0.0047 - Jaccard_index: 0.8006 - Specificity: 0.9753 - Sensitivity: 1.0030 - precisio - ETA: 2:03 - loss: -0.8810 - dice_coef: 0.8888 - acc: 0.8417 - mean_squared_error: 0.0047 - Jaccard_index: 0.8008 - Spec - ETA: 1:55 - loss: -0.8810 - dice_coef: 0.8889 - acc: 0.8417 - mean_squared_error: 0.0047 - Jaccard_inde - ETA: 1:24 - loss: -0.8811 - dice_coef: 0.8889 - acc: 0.8399 - mean_squared_error: 0.0047 - Jaccard_index: 0.8009 - Specificity: 0.9752 - Sensitivity: 1.0030 - precision: 0.9652 - recall - ETA: 1:22 - loss: -0.8810 - dice_coef: 0.8889 - acc: 0.8395 - mean_squared_error: 0.0047 - Jaccard_inde - ETA: 43s - loss: -0.8805 - dice_coef: 0.8883 - acc: 0.8388 - mean_squared_error: 0.0047 - Jaccard_index: 0.8000 - Specificity: 0.9 - ETA: 28s - loss: -0.8805 - dice_coef: 0.8883 - acc: 0.838 - ETA: 5s - loss: -0.8804 - dice_coef: 0.8882 - acc: 0.8389 - mean_squared_error: 0.0047 - Jaccard_index: 0.7997 - Specificity: 0.9751 - Sensitivity: 1.0030 \n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score:  -0.7249414658546448\n",
      "Validation Accuracy:  0.732685718536377\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate_generator(validation_generator, steps=50)\n",
    "\n",
    "print ('Validation Score: ', score[0])\n",
    "print ('Validation Accuracy: ',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 5s 179ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_generator(test_generator, verbose=1, steps=nb_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 256, 256, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'dice_coef', 'acc', 'mean_squared_error', 'Jaccard_index', 'Specificity', 'Sensitivity', 'precision', 'recall', 'f1score']\n",
      "[-0.7724999097415379, 0.7802441654886518, 0.9798227146693639, 0.01984564715198108, 0.6433794982092721, 0.9680362166677202, 0.9987403883252826, 0.8569075809206281, 0.721968252658844, 0.7803062282289778]\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate_generator(test_generator, steps=50)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training co-effiency    : 0.7998570555127135;\n",
      "Validation co-effiency : 0.5957816480932867\n"
     ]
    }
   ],
   "source": [
    "coef = np.array(history.history['Jaccard_index'])\n",
    "val_coef = np.array(history.history['val_Jaccard_index'])\n",
    "print(\"Training co-effiency    : {};\\nValidation co-effiency : {}\".format(coef[coef==max(coef)][0], val_coef[np.argmax(coef)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "\n",
    "def labelVisualize(num_class,color_dict,img):\n",
    "    img = img[:,:,0] if len(img.shape) == 3 else img\n",
    "    img_out = np.zeros(img.shape + (3,))\n",
    "    for i in range(num_class):\n",
    "        img_out[img == i,:] = color_dict[i]\n",
    "    return img_out / 255\n",
    "\n",
    "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)\n",
    "\n",
    "os.mkdir('05.U-Net2_results_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveResult('05.U-Net2_results_dir',prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(img,mask):\n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    img -= mean\n",
    "    img /= std\n",
    "    mask = mask /255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    return (img,mask)\n",
    "\n",
    "test_data_dir= \"data_augmented/cvc300_test/images/\"\n",
    "test_data_mask_dir = \"data_augmented/cvc300_test/masks/\"\n",
    "test_data_pred_dir = \"05.U-Net2_results_dir\"\n",
    "img_rows, img_cols = 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "file_names = next(os.walk(test_data_dir))[2]\n",
    "scores = []\n",
    "for file in file_names:\n",
    "    grey_img = load_img(os.path.join(test_data_dir,file), target_size=(img_rows, img_cols), grayscale=False)\n",
    "    mask_img = load_img(os.path.join(test_data_mask_dir,file), target_size=(img_rows, img_cols), grayscale=True)\n",
    "    img = img_to_array(grey_img)\n",
    "    img_mask = img_to_array(mask_img)\n",
    "    \n",
    "    img, img_mask = normalizeData(img, img_mask)\n",
    "    img = np.reshape(img,(1,)+img.shape)\n",
    "    \n",
    "    pred = model.predict([img])\n",
    "    sess = tf.Session()\n",
    "    score = sess.run(Jaccard_index(img_mask, pred))\n",
    "    print(\"{} -- jaccard index: {}\".format(file,score))\n",
    "    scores.append([file,score])\n",
    "\n",
    "    result_img = array_to_img(pred[0] * 255 )\n",
    "    result_img.save(os.path.join(test_data_pred_dir, file.split('.')[0] + '_predict.jpg'))\n",
    "\n",
    "with open(\"unet_test_result.csv\", 'w') as f:\n",
    "    f.write(\"filename, jaccard_index\\n\")\n",
    "    for i in range(len(scores)):\n",
    "        #print(scores[i])\n",
    "        f.write(\"{},{}\\n\".format(scores[i][0], scores[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('05.U-Net2_results_dir/102_predict.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(prediction[1]), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(prediction[1]), cmap = \"gray\", vmin = 0.0, vmax= 0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ =  np.squeeze(np.multiply(prediction[1], 255))\n",
    "print(results_)\n",
    "\n",
    "plt.imshow(results_, cmap = \"gray\", vmin = 0.0, vmax= 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "img=mpimg.imread('05.U-Net2_results_dir/35_predict.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['dice_coef'])\n",
    "plt.plot(history.history['val_dice_coef'])\n",
    "plt.title('model dice_coeff')\n",
    "plt.ylabel('dice_coef')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
